{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "import helper \n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data, get base paired indices and remove gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and dict construction completed in: 10.93s\n"
     ]
    }
   ],
   "source": [
    "starttime = time.time()\n",
    "\n",
    "#Open data from h5py\n",
    "\n",
    "data_path = '../../../data_RFAM/glna_100k_d8.hdf5'\n",
    "with h5py.File(data_path, 'r') as dataset:\n",
    "    X_data = np.array(dataset['X_data'])\n",
    "    Y_data = np.array(dataset['Y_data'])\n",
    "    \n",
    "numdata, seqlen, _, dims = X_data.shape\n",
    "dims = dims-1\n",
    "\n",
    "#remove gaps from sequences\n",
    "ungapped = True\n",
    "if ungapped:\n",
    "    X_data = X_data[:, :, :, :dims]\n",
    "    \n",
    "# get validation and test set from training set\n",
    "test_frac = 0.3\n",
    "valid_frac = 0.1\n",
    "N = numdata\n",
    "split_1 = int(N*(1-valid_frac-test_frac))\n",
    "split_2 = int(N*(1-test_frac))\n",
    "shuffle = np.random.permutation(N)\n",
    "\n",
    "X_train = X_data[shuffle[:split_1], :, 0, :]\n",
    "X_valid = X_data[shuffle[split_1:split_2], :, 0, :]\n",
    "X_test = X_data[shuffle[split_2:], :, 0, :]\n",
    "\n",
    "Y_train = Y_data[shuffle[:split_1]]\n",
    "Y_valid = Y_data[shuffle[split_1:split_2]]\n",
    "Y_test = Y_data[shuffle[split_2:]]\n",
    "    \n",
    "print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "simalign_file = '../../../data_RFAM/glnAsim_100k.sto'\n",
    "\n",
    "#Get the full secondary structure and sequence consensus from the emission\n",
    "SS = mf.getSSconsensus(simalign_file)\n",
    "SQ = mf.getSQconsensus(simalign_file)\n",
    "\n",
    "#Get the ungapped sequence and the indices of ungapped nucleotides\n",
    "_, ugSS, ugidx = mf.rm_consensus_gaps(X_data, SS)\n",
    "_, ugSQ, _ = mf.rm_consensus_gaps(X_data, SQ)\n",
    "\n",
    "\n",
    "#Get the sequence and indices of the conserved base pairs\n",
    "bpchars = ['(',')','<','>','{','}']\n",
    "sig_bpchars = ['<','>']\n",
    "bpidx, bpSS, nonbpidx = mf.sigbasepair(SS, bpchars)\n",
    "numbp = len(bpidx)\n",
    "numug = len(ugidx)\n",
    "\n",
    "#Get the bpug information\n",
    "bpugSQ, bpugidx = mf.bpug(ugidx, bpidx, SQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unalign(X):\n",
    "    nuc_index = np.where(np.sum(X, axis=1)!=0)\n",
    "    return (X[nuc_index])\n",
    "\n",
    "X_train_unalign = [unalign(X) for X in X_train]\n",
    "X_valid_unalign = [unalign(X) for X in X_valid]\n",
    "X_test_unalign = [unalign(X) for X in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000, 893, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the 2 layer LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "num_hidden = 64\n",
    "num_layers = 2\n",
    "num_classes = Y_train.shape[1]\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, None, X_train[0].shape[1]], name='inputs')\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes], name='ouputs')\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "lstm1_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "lstm1_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(lstm1_fw_cell, lstm1_bw_cell, X, \n",
    "                                                   sequence_length=helper.length(X), dtype=tf.float32,\n",
    "                                                   scope='BLSTM_1')\n",
    "\n",
    "outputs_forward, outputs_backward = outputs1\n",
    "\n",
    "# states_forward is a tuple of (c is the hidden state and h is the output)\n",
    "concat_outputs = tf.concat([outputs_forward, outputs_backward], axis=2, name='intermediate')\n",
    "\n",
    "lstm2_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "lstm2_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "outputs2, states2 = tf.nn.bidirectional_dynamic_rnn(lstm2_fw_cell, lstm2_bw_cell, concat_outputs,\n",
    "                                                    scope='BLSTM_2', dtype=tf.float32)\n",
    "\n",
    "states_forward, states_backward = states2\n",
    "\n",
    "# states_forward is a tuple of (c is the hidden state and h is the output)\n",
    "concat_states = tf.concat([states_forward[1], states_backward[1]], axis=1, name='output')\n",
    "\n",
    "# Linear activation, using rnn inner loop last output\n",
    "W_out = tf.Variable(tf.random_normal([num_hidden*2, num_classes]))\n",
    "b_out = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "#last = tf.gather(outputs, int(outputs.get_shape()[1])-1)  \n",
    "#last = int(outputs.get_shape()[1]) - 1\n",
    "logits = tf.matmul(concat_states, W_out) + b_out\n",
    "predictions = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The Optimizer\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "# Define loss and optimizer\n",
    "predictions = tf.clip_by_value(predictions, clip_value_max=1-1e-7, clip_value_min=1e-7)\n",
    "#cost = tf.reduce_sum(Y*tf.log(predictions), axis=1)\n",
    "cost = tf.reduce_sum(Y*tf.log(predictions)+(1-Y)*tf.log(1-predictions), axis=1)\n",
    "\n",
    "total_loss = tf.reduce_mean(-cost)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "grads = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "# Apply gradients.\n",
    "apply_gradient_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "# Track the moving averages of all trainable variables.\n",
    "variable_averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "    train_op = tf.no_op(name='train')\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "#correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(Y, 1))\n",
    "correct_pred = tf.equal(tf.round(predictions), Y)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# start session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1\n",
      "[=========================] 99.9% -- elapsed time=106.34s -- loss=0.46147 -- acc=0.759775\n",
      "\n",
      "  valid loss  = 0.33825167287497004\n",
      "  valid acc   = 0.855\n",
      "  valid AUROC = 0.9303328895805599\n",
      "  valid AUPRC = 0.9396899817830835\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 2\n",
      "[=========================] 99.9% -- elapsed time=103.83s -- loss=0.31561 -- acc=0.8673411\n",
      "\n",
      "  valid loss  = 0.2917653535534242\n",
      "  valid acc   = 0.88025\n",
      "  valid AUROC = 0.9474347814673842\n",
      "  valid AUPRC = 0.9527089886360935\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 3\n",
      "[=========================] 99.9% -- elapsed time=105.92s -- loss=0.27736 -- acc=0.887442\n",
      "\n",
      "  valid loss  = 0.25542690682041036\n",
      "  valid acc   = 0.8975\n",
      "  valid AUROC = 0.9584695504346903\n",
      "  valid AUPRC = 0.9636545442805443\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 4\n",
      "[=========================] 99.9% -- elapsed time=103.17s -- loss=0.24183 -- acc=0.904788\n",
      "\n",
      "  valid loss  = 0.22684296387823144\n",
      "  valid acc   = 0.9097\n",
      "  valid AUROC = 0.9674254414090089\n",
      "  valid AUPRC = 0.9718762736059432\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 5\n",
      "[=========================] 99.9% -- elapsed time=103.70s -- loss=0.21525 -- acc=0.917024\n",
      "\n",
      "  valid loss  = 0.20323747745270182\n",
      "  valid acc   = 0.92295\n",
      "  valid AUROC = 0.9729686610541316\n",
      "  valid AUPRC = 0.9766935662057771\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 6\n",
      "[=========================] 99.9% -- elapsed time=103.31s -- loss=0.19581 -- acc=0.926964\n",
      "\n",
      "  valid loss  = 0.17957665558287483\n",
      "  valid acc   = 0.93315\n",
      "  valid AUROC = 0.9782462740392127\n",
      "  valid AUPRC = 0.9815222997992079\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 7\n",
      "[=========================] 99.9% -- elapsed time=103.28s -- loss=0.18040 -- acc=0.932993\n",
      "\n",
      "  valid loss  = 0.19055006248508669\n",
      "  valid acc   = 0.92805\n",
      "  valid AUROC = 0.9784624753904712\n",
      "  valid AUPRC = 0.9821273143016238\n",
      "epoch: 8\n",
      "[=========================] 99.9% -- elapsed time=103.70s -- loss=0.16633 -- acc=0.938513\n",
      "\n",
      "  valid loss  = 0.15631929243422427\n",
      "  valid acc   = 0.94235\n",
      "  valid AUROC = 0.9828975081094257\n",
      "  valid AUPRC = 0.9857178783550791\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 9\n",
      "[=========================] 99.9% -- elapsed time=103.70s -- loss=0.15892 -- acc=0.941977\n",
      "\n",
      "  valid loss  = 0.1518302766374864\n",
      "  valid acc   = 0.944\n",
      "  valid AUROC = 0.9842230913943214\n",
      "  valid AUPRC = 0.9868037578256408\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 10\n",
      "[=========================] 99.9% -- elapsed time=103.46s -- loss=0.15013 -- acc=0.945031\n",
      "\n",
      "  valid loss  = 0.14374657966505\n",
      "  valid acc   = 0.94775\n",
      "  valid AUROC = 0.9852809580059876\n",
      "  valid AUPRC = 0.9878152617379914\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 11\n",
      "[=========================] 99.9% -- elapsed time=103.73s -- loss=0.14297 -- acc=0.947704\n",
      "\n",
      "  valid loss  = 0.14547088417034762\n",
      "  valid acc   = 0.9479\n",
      "  valid AUROC = 0.985868431677698\n",
      "  valid AUPRC = 0.9879834806708802\n",
      "epoch: 12\n",
      "[=========================] 99.9% -- elapsed time=103.92s -- loss=0.14010 -- acc=0.948886\n",
      "\n",
      "  valid loss  = 0.16045304896624368\n",
      "  valid acc   = 0.9412\n",
      "  valid AUROC = 0.9867473871711697\n",
      "  valid AUPRC = 0.9884955905415151\n",
      "epoch: 13\n",
      "[=========================] 99.9% -- elapsed time=103.57s -- loss=0.13456 -- acc=0.950151\n",
      "\n",
      "  valid loss  = 0.14651135419003047\n",
      "  valid acc   = 0.9451\n",
      "  valid AUROC = 0.9861721435758974\n",
      "  valid AUPRC = 0.9878822975545384\n",
      "epoch: 14\n",
      "[=========================] 99.9% -- elapsed time=103.64s -- loss=0.13091 -- acc=0.951642\n",
      "\n",
      "  valid loss  = 0.13081131356910677\n",
      "  valid acc   = 0.952\n",
      "  valid AUROC = 0.9881017156357227\n",
      "  valid AUPRC = 0.9898998533788226\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 15\n",
      "[=========================] 99.9% -- elapsed time=103.74s -- loss=0.12829 -- acc=0.952872\n",
      "\n",
      "  valid loss  = 0.13302761240666555\n",
      "  valid acc   = 0.9518\n",
      "  valid AUROC = 0.9878102788142425\n",
      "  valid AUPRC = 0.9895761661586286\n",
      "epoch: 16\n",
      "[=========================] 99.9% -- elapsed time=103.17s -- loss=0.12199 -- acc=0.954724\n",
      "\n",
      "  valid loss  = 0.1263499928691727\n",
      "  valid acc   = 0.95485\n",
      "  valid AUROC = 0.9886862792892456\n",
      "  valid AUPRC = 0.9904105777744262\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 17\n",
      "[=========================] 99.9% -- elapsed time=103.33s -- loss=0.11990 -- acc=0.955420\n",
      "\n",
      "  valid loss  = 0.12370529007997103\n",
      "  valid acc   = 0.9542\n",
      "  valid AUROC = 0.9894992893705585\n",
      "  valid AUPRC = 0.9910019863239562\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 18\n",
      "[=========================] 99.9% -- elapsed time=103.27s -- loss=0.11798 -- acc=0.955770\n",
      "\n",
      "  valid loss  = 0.12263150039354374\n",
      "  valid acc   = 0.9548\n",
      "  valid AUROC = 0.9897968012300078\n",
      "  valid AUPRC = 0.991117467079164\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 19\n",
      "[=========================] 99.9% -- elapsed time=103.47s -- loss=0.11357 -- acc=0.958080\n",
      "\n",
      "  valid loss  = 0.11974792938922668\n",
      "  valid acc   = 0.9562\n",
      "  valid AUROC = 0.990248774054838\n",
      "  valid AUPRC = 0.9916341912362882\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 20\n",
      "[=========================] 99.9% -- elapsed time=103.11s -- loss=0.11097 -- acc=0.958805\n",
      "\n",
      "  valid loss  = 0.11953783161036528\n",
      "  valid acc   = 0.9569\n",
      "  valid AUROC = 0.9902513540709629\n",
      "  valid AUPRC = 0.9915705667483615\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 21\n",
      "[=========================] 99.9% -- elapsed time=103.54s -- loss=0.10870 -- acc=0.959682\n",
      "\n",
      "  valid loss  = 0.11576575592195817\n",
      "  valid acc   = 0.95725\n",
      "  valid AUROC = 0.990618611366321\n",
      "  valid AUPRC = 0.9919274761160614\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 22\n",
      "[=========================] 99.9% -- elapsed time=103.50s -- loss=0.10638 -- acc=0.960274\n",
      "\n",
      "  valid loss  = 0.1335000156129823\n",
      "  valid acc   = 0.94995\n",
      "  valid AUROC = 0.9901126532040825\n",
      "  valid AUPRC = 0.9912729218772541\n",
      "epoch: 23\n",
      "[=========================] 99.9% -- elapsed time=103.74s -- loss=0.10282 -- acc=0.961180\n",
      "\n",
      "  valid loss  = 0.11399537737327065\n",
      "  valid acc   = 0.95755\n",
      "  valid AUROC = 0.9912356052225327\n",
      "  valid AUPRC = 0.9923573970909264\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 24\n",
      "[=========================] 99.9% -- elapsed time=103.62s -- loss=0.09973 -- acc=0.962267\n",
      "\n",
      "  valid loss  = 0.12839580857351415\n",
      "  valid acc   = 0.9526\n",
      "  valid AUROC = 0.99004989281183\n",
      "  valid AUPRC = 0.9913568862228089\n",
      "epoch: 25\n",
      "[=========================] 99.9% -- elapsed time=103.42s -- loss=0.09735 -- acc=0.963830\n",
      "\n",
      "  valid loss  = 0.11473025850748501\n",
      "  valid acc   = 0.9578\n",
      "  valid AUROC = 0.9916555028468929\n",
      "  valid AUPRC = 0.9926949397034386\n",
      "epoch: 26\n",
      "[=========================] 99.9% -- elapsed time=103.56s -- loss=0.09418 -- acc=0.964500\n",
      "\n",
      "  valid loss  = 0.11090031304696255\n",
      "  valid acc   = 0.95865\n",
      "  valid AUROC = 0.9919365746035913\n",
      "  valid AUPRC = 0.9929646234828622\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 27\n",
      "[=========================] 99.9% -- elapsed time=103.27s -- loss=0.09140 -- acc=0.965205\n",
      "\n",
      "  valid loss  = 0.1106904355789072\n",
      "  valid acc   = 0.9599\n",
      "  valid AUROC = 0.9919021343883399\n",
      "  valid AUPRC = 0.9928932236341139\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 28\n",
      "[=========================] 99.9% -- elapsed time=103.51s -- loss=0.09033 -- acc=0.965938\n",
      "\n",
      "  valid loss  = 0.11050756967101531\n",
      "  valid acc   = 0.9588\n",
      "  valid AUROC = 0.9925300033125208\n",
      "  valid AUPRC = 0.9934417313301209\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 29\n",
      "[=========================] 99.9% -- elapsed time=103.60s -- loss=0.08724 -- acc=0.966786\n",
      "\n",
      "  valid loss  = 0.10542587956167448\n",
      "  valid acc   = 0.9622\n",
      "  valid AUROC = 0.99244397277483\n",
      "  valid AUPRC = 0.9933490188688824\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 30\n",
      "[=========================] 99.9% -- elapsed time=103.52s -- loss=0.08359 -- acc=0.968591\n",
      "\n",
      "  valid loss  = 0.1087307248091782\n",
      "  valid acc   = 0.9613\n",
      "  valid AUROC = 0.9922035762723516\n",
      "  valid AUPRC = 0.9932122502676237\n",
      "epoch: 31\n",
      "[=========================] 99.9% -- elapsed time=103.35s -- loss=0.08142 -- acc=0.969466\n",
      "\n",
      "  valid loss  = 0.10979819685195878\n",
      "  valid acc   = 0.9613\n",
      "  valid AUROC = 0.992195246220289\n",
      "  valid AUPRC = 0.9931780157108983\n",
      "epoch: 32\n",
      "[=========================] 99.9% -- elapsed time=103.39s -- loss=0.07813 -- acc=0.970135\n",
      "\n",
      "  valid loss  = 0.10505770264337266\n",
      "  valid acc   = 0.96225\n",
      "  valid AUROC = 0.9927242945268409\n",
      "  valid AUPRC = 0.9935549799106154\n",
      "  Lower validation loss found. Saving parameters to: ../../../results/glnA/RNN_t3_best\n",
      "epoch: 33\n",
      "[=========================] 99.9% -- elapsed time=103.45s -- loss=0.07633 -- acc=0.971262\n",
      "\n",
      "  valid loss  = 0.10576852258709318\n",
      "  valid acc   = 0.9619\n",
      "  valid AUROC = 0.9929279557997238\n",
      "  valid AUPRC = 0.9938222100884633\n",
      "epoch: 34\n",
      "[=========================] 99.9% -- elapsed time=103.35s -- loss=0.08064 -- acc=0.969524\n",
      "\n",
      "  valid loss  = 0.10908925068523188\n",
      "  valid acc   = 0.9602\n",
      "  valid AUROC = 0.9925305833161457\n",
      "  valid AUPRC = 0.9933128791797085\n",
      "epoch: 35\n",
      "[=========================] 99.9% -- elapsed time=103.68s -- loss=0.07320 -- acc=0.972449\n",
      "\n",
      "  valid loss  = 0.10558506513494927\n",
      "  valid acc   = 0.96305\n",
      "  valid AUROC = 0.9928647954049713\n",
      "  valid AUPRC = 0.9936882558694152\n",
      "epoch: 36\n",
      "[=========================] 99.9% -- elapsed time=104.20s -- loss=0.06916 -- acc=0.973712\n",
      "\n",
      "  valid loss  = 0.11036566195039636\n",
      "  valid acc   = 0.96265\n",
      "  valid AUROC = 0.992771779823624\n",
      "  valid AUPRC = 0.9935783422959606\n",
      "epoch: 37\n",
      "[=========================] 99.9% -- elapsed time=107.99s -- loss=0.06735 -- acc=0.974525\n",
      "\n",
      "  valid loss  = 0.10966272670130547\n",
      "  valid acc   = 0.9627\n",
      "  valid AUROC = 0.9926464140400878\n",
      "  valid AUPRC = 0.9935189930814474\n",
      "epoch: 38\n",
      "[=========================] 99.9% -- elapsed time=107.98s -- loss=0.06547 -- acc=0.975359\n",
      "\n",
      "  valid loss  = 0.10983813965376496\n",
      "  valid acc   = 0.96235\n",
      "  valid AUROC = 0.9927373796086224\n",
      "  valid AUPRC = 0.9935954402533484\n",
      "epoch: 39\n",
      "[=========================] 99.9% -- elapsed time=107.99s -- loss=0.06302 -- acc=0.976517\n",
      "\n",
      "  valid loss  = 0.11087839065641825\n",
      "  valid acc   = 0.9612\n",
      "  valid AUROC = 0.992762209763811\n",
      "  valid AUPRC = 0.9935236853957514\n",
      "epoch: 40\n",
      "[=========================] 99.9% -- elapsed time=108.03s -- loss=0.06211 -- acc=0.976511\n",
      "\n",
      "  valid loss  = 0.11663910967689363\n",
      "  valid acc   = 0.9616\n",
      "  valid AUROC = 0.9932680779254871\n",
      "  valid AUPRC = 0.9938792840881886\n",
      "epoch: 41\n",
      "[=========================] 99.9% -- elapsed time=107.90s -- loss=0.05880 -- acc=0.978065\n",
      "\n",
      "  valid loss  = 0.11533367694993236\n",
      "  valid acc   = 0.9613\n",
      "  valid AUROC = 0.9926293789336185\n",
      "  valid AUPRC = 0.9933079990494705\n",
      "epoch: 42\n",
      "[=========================] 99.9% -- elapsed time=108.38s -- loss=0.05653 -- acc=0.978545\n",
      "\n",
      "  valid loss  = 0.11493040898675395\n",
      "  valid acc   = 0.96195\n",
      "  valid AUROC = 0.9926731142069637\n",
      "  valid AUPRC = 0.9934776376736765\n",
      "Patience ran out... early stopping!\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "train_batches = helper.bucket_generator(X_train_unalign, Y_train, batch_size)\n",
    "valid_batches = helper.bucket_generator(X_valid_unalign, Y_valid, batch_size)\n",
    "num_epochs = 50\n",
    "bar_length = 25\n",
    "patience = 10\n",
    "\n",
    "\n",
    "# path to save results\n",
    "save_path = '../../../results/glnA'\n",
    "if not os.path.isdir(save_path):\n",
    "    os.mkdir(save_path)\n",
    "    print(\"making directory: \" + save_path)\n",
    "params_filename = 'RNN_t3' + '_best'\n",
    "params_path = os.path.join(save_path, params_filename)\n",
    "    \n",
    "wait=0\n",
    "min_loss = 1e10\n",
    "for epoch in range(num_epochs):\n",
    "    print('epoch: '+ str(epoch+1))\n",
    "    \n",
    "    num_batches = len(train_batches)\n",
    "    shuffled_batches = []\n",
    "    for i in np.random.permutation(num_batches):\n",
    "        shuffled_batches.append(train_batches[i])\n",
    "        \n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(shuffled_batches):\n",
    "        batch_loss, batch_acc, _ = sess.run([total_loss, accuracy, train_op], feed_dict={X: batch[0], \n",
    "                                                                                      Y: batch[1], \n",
    "                                                                                      keep_prob: 0.5,\n",
    "                                                                                      learning_rate: 0.0003})            \n",
    "        loss += batch_loss\n",
    "        acc += batch_acc\n",
    "\n",
    "        remaining_time = (time.time()-start_time)*(num_batches-(i+1))/(i+1)\n",
    "        percent = float(i)/num_batches\n",
    "        progress = '='*int(round(percent*bar_length))\n",
    "        spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "        sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs -- loss=%.5f -- acc=%.5f\" \\\n",
    "        %(progress+spaces, percent*100, remaining_time, loss/(i+1), acc/(i+1)))\n",
    "        \n",
    "    sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs -- loss=%.5f -- acc=%.5f\\n\" \\\n",
    "    %(progress+spaces, percent*100, time.time()-start_time, loss/(i+1), acc/(i+1)))\n",
    "    sys.stdout.write(\"\\n\")\n",
    "    \n",
    "    \n",
    "    num_batches = len(valid_batches)\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    valid_predictions = []\n",
    "    valid_truth = []\n",
    "    start_time = time.time()\n",
    "    for i, batch in enumerate(valid_batches):\n",
    "        batch_loss, batch_predict = sess.run([total_loss, predictions], feed_dict={X: batch[0], \n",
    "                                                                                Y: batch[1], \n",
    "                                                                                keep_prob: 1.0})            \n",
    "        loss += batch_loss\n",
    "        valid_predictions.append(batch_predict)\n",
    "        valid_truth.append(batch[1])\n",
    "    valid_loss = loss/num_batches\n",
    "    valid_predictions = np.vstack(valid_predictions)\n",
    "    valid_truth = np.vstack(valid_truth)\n",
    "    \n",
    "    correct = np.mean(np.equal(valid_truth, np.round(valid_predictions)))\n",
    "    auc_roc, roc_curves = helper.roc(valid_truth, valid_predictions)\n",
    "    auc_pr, pr_curves = helper.pr(valid_truth, valid_predictions)\n",
    "    print(\"  valid loss  = \"+str(loss/num_batches))\n",
    "    print(\"  valid acc   = \"+str(np.nanmean(correct)))\n",
    "    print(\"  valid AUROC = \"+str(np.nanmean(auc_roc)))\n",
    "    print(\"  valid AUPRC = \"+str(np.nanmean(auc_pr)))\n",
    "    \n",
    "    # check if current validation loss is lower, if so, save parameters, if not check patience\n",
    "    if valid_loss < min_loss:\n",
    "        print(\"  Lower validation loss found. Saving parameters to: \"+params_path)\n",
    "        \n",
    "        # save model parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.save(sess, save_path=params_path)\n",
    "        \n",
    "        # set minimum loss to the current validation loss\n",
    "        min_loss = valid_loss\n",
    "        \n",
    "        # reset wait time\n",
    "        wait = 0\n",
    "    else:\n",
    "        \n",
    "        # add to wait time\n",
    "        wait += 1\n",
    "        \n",
    "        # check to see if patience has run out\n",
    "        if wait == patience:\n",
    "            print(\"Patience ran out... early stopping!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close tensorflow session (Note, the graph is still open)\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../../../results/glnA/RNN_t3_best\n"
     ]
    }
   ],
   "source": [
    "# create a new session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# path to save results\n",
    "save_path = '../../../results/glnA'\n",
    "params_filename = 'RNN_t3' + '_best'\n",
    "params_path = os.path.join(save_path, params_filename)\n",
    "\n",
    "# restore trained parameters\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess, save_path=params_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bucket_generator(X_train, Y_train, batch_size=32):\n",
    "    num_data = len(X_train)\n",
    "\n",
    "    # add zero padding\n",
    "    lengths = []\n",
    "    for x in X_train:\n",
    "        lengths.append(x.shape[0])\n",
    "    lengths = np.array(lengths)\n",
    "\n",
    "    # sort the lengths\n",
    "    sort_index = np.argsort(lengths)\n",
    "\n",
    "    num_buckets = np.floor(num_data/batch_size).astype(int)\n",
    "    buckets = []\n",
    "    for i in range(num_buckets):\n",
    "        indices = sort_index[i*batch_size:(i+1)*batch_size]\n",
    "        X = []\n",
    "        for j in indices:\n",
    "            X.append(X_train[j])\n",
    "\n",
    "        MAX = len(X[-1])\n",
    "        X_padded, _ = helper.pad_inputs(X, MAX)\n",
    "        buckets.append([X_padded, \n",
    "                        Y_train[indices,:]])\n",
    "\n",
    "    if num_data > num_buckets*batch_size:\n",
    "        indices = sort_index[num_buckets*batch_size:]\n",
    "        X = []\n",
    "        for j in indices:\n",
    "            X.append(X_train[j])\n",
    "\n",
    "        MAX = len(X[-1])\n",
    "        X_padded, _ = helper.pad_inputs(X, MAX)\n",
    "        buckets.append([X_padded, \n",
    "                        Y_train[indices,:]])\n",
    "\n",
    "    return buckets, sort_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "batches, index = bucket_generator(X_test_unalign, Y_test, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([39776, 30981,  3976, ..., 49413, 39070, 18752])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[=========================] 99.8% -- elapsed time=17.91s -- loss=0.08459 -- acc=0.0000000\n",
      "\n",
      "  test loss  = 0.0845856097933185\n",
      "  test acc   = 0.9684166666666667\n",
      "  test AUROC = 0.9950403854367609\n",
      "  test AUPRC = 0.9956391309874065\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "batches, index = bucket_generator(X_test_unalign, Y_test, batch_size)\n",
    "num_batches = len(batches)\n",
    "\n",
    "loss = 0\n",
    "acc = 0\n",
    "valid_predictions = []\n",
    "valid_truth = []\n",
    "start_time = time.time()\n",
    "num_batches = len(batches)\n",
    "bar_length = 25\n",
    "\n",
    "for i, batch in enumerate(batches):\n",
    "\n",
    "    batch_loss, batch_predict = sess.run([total_loss, predictions], feed_dict={X: batch[0], \n",
    "                                                                            Y: batch[1], \n",
    "                                                                            keep_prob: 1.0})            \n",
    "    loss += batch_loss\n",
    "    valid_predictions.append(batch_predict)\n",
    "    valid_truth.append(batch[1])\n",
    "    \n",
    "    remaining_time = (time.time()-start_time)*(num_batches-(i+1))/(i+1)\n",
    "    percent = float(i)/num_batches\n",
    "    progress = '='*int(round(percent*bar_length))\n",
    "    spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "    sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs -- loss=%.5f -- acc=%.5f\" \\\n",
    "    %(progress+spaces, percent*100, remaining_time, loss/(i+1), acc/(i+1)))\n",
    "\n",
    "sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs -- loss=%.5f -- acc=%.5f\\n\" \\\n",
    "%(progress+spaces, percent*100, time.time()-start_time, loss/(i+1), acc/(i+1)))\n",
    "sys.stdout.write(\"\\n\")\n",
    "\n",
    "valid_predictions = np.vstack(valid_predictions)\n",
    "valid_truth = np.vstack(valid_truth)\n",
    "\n",
    "correct = np.mean(np.equal(valid_truth, np.round(valid_predictions)))\n",
    "auc_roc, roc_curves = helper.roc(valid_truth, valid_predictions)\n",
    "auc_pr, pr_curves = helper.pr(valid_truth, valid_predictions)\n",
    "mean = [np.nanmean(correct), np.nanmean(auc_roc), np.nanmean(auc_pr)]\n",
    "std = [np.nanstd(correct), np.nanstd(auc_roc), np.nanstd(auc_pr)]\n",
    "\n",
    "print(\"  test loss  = \"+str(loss/num_batches))\n",
    "print(\"  test acc   = \"+str(np.nanmean(correct)))\n",
    "print(\"  test AUROC = \"+str(np.nanmean(auc_roc)))\n",
    "print(\"  test AUPRC = \"+str(np.nanmean(auc_pr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WT_predictions = valid_predictions[np.argsort(index)]\n",
    "plot_index = np.argsort(WT_predictions[:,0])[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
