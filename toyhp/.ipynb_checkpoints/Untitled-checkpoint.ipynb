{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TEST = True\n",
    "FOM = True\n",
    "WRITE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and dict construction completed in: 0.18s\n",
      "loading model from:  ../../results/toyhp/resbind_small_best.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/resbind_small_best.ckpt\n",
      "  test  loss:\t\t0.19882\n",
      "  test  accuracy:\t0.93187+/-0.00000\n",
      "  test  auc-roc:\t0.98273+/-0.00000\n",
      "  test  auc-pr:\t\t0.97589+/-0.00000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WRITE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-62d0b890cd7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnntrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mWRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             \u001b[0mmetricsline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s,%s,%s,%s,%s,%s,%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodelarch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test_metrics.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WRITE' is not defined"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "from deepomics import neuralnetwork as nn\n",
    "from deepomics import utils, fit, visualize, saliency\n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE LOOP'''\n",
    "trials = ['med']#['small', 'med', 'large']\n",
    "exp = 'toyhp'  #for both the data folder and the params folder\n",
    "exp_data = 'data_%s'%(exp)\n",
    "\n",
    "for t in trials:\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    '''OPEN DATA'''\n",
    "\n",
    "    starttime = time.time()\n",
    "\n",
    "    #Open data from h5py\n",
    "    filename = '%s_50k_%s.hdf5'%(exp, t)\n",
    "    data_path = os.path.join('../..', exp_data, filename)\n",
    "    with h5py.File(data_path, 'r') as dataset:\n",
    "        X_data = np.array(dataset['X_data'])\n",
    "        Y_data = np.array(dataset['Y_data'])\n",
    "\n",
    "    numdata, seqlen, dims = X_data.shape\n",
    "    X_data = np.expand_dims(X_data, axis=2)\n",
    "\n",
    "    # get validation and test set from training set\n",
    "    test_frac = 0.3\n",
    "    valid_frac = 0.1\n",
    "    N = numdata\n",
    "    split_1 = int(N*(1-valid_frac-test_frac))\n",
    "    split_2 = int(N*(1-test_frac))\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    #set up dictionaries\n",
    "    train = {'inputs': X_data[shuffle[:split_1]], \n",
    "           'targets': Y_data[shuffle[:split_1]]}\n",
    "    valid = {'inputs': X_data[shuffle[split_1:split_2]], \n",
    "           'targets': Y_data[shuffle[split_1:split_2]]}\n",
    "    test = {'inputs': X_data[shuffle[split_2:]], \n",
    "           'targets': Y_data[shuffle[split_2:]]}\n",
    "\n",
    "    print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    '''SAVE PATHS AND PARAMETERS'''\n",
    "    params_results = '../../results'\n",
    "\n",
    "    modelarch = 'resbind'\n",
    "    trial = t\n",
    "    modelsavename = '%s_%s'%(modelarch, trial)\n",
    "\n",
    "\n",
    "\n",
    "    '''BUILD NEURAL NETWORK'''\n",
    "\n",
    "    def cnn_model(input_shape, output_shape):\n",
    "\n",
    "        # create model\n",
    "        layer1 = {'layer': 'input', #41\n",
    "              'input_shape': input_shape\n",
    "              }\n",
    "        layer2 = {'layer': 'conv1d',\n",
    "              'num_filters': 96,\n",
    "              'filter_size': input_shape[1]-29,\n",
    "              'norm': 'batch',\n",
    "              'activation': 'relu',\n",
    "              'dropout': 0.3,\n",
    "              'padding': 'VALID',\n",
    "              }\n",
    "        layer3 = {'layer': 'conv1d_residual',\n",
    "              'filter_size': 5,\n",
    "              'function': 'relu',\n",
    "              'dropout_block': 0.1,\n",
    "              'dropout': 0.3,\n",
    "              'mean_pool': 10,\n",
    "              }\n",
    "\n",
    "        layer4 = {'layer': 'dense',        # input, conv1d, dense, conv1d_residual, dense_residual, conv1d_transpose,\n",
    "                                      # concat, embedding, variational_normal, variational_softmax, + more\n",
    "            'num_units': 196,\n",
    "            'norm': 'batch',          # if removed, automatically adds bias instead\n",
    "            'activation': 'relu',     # or leaky_relu, prelu, sigmoid, tanh, etc\n",
    "            'dropout': 0.5,           # if removed, default is no dropout\n",
    "               }\n",
    "\n",
    "\n",
    "        layer5 = {'layer': 'dense',\n",
    "              'num_units': output_shape[1],\n",
    "              'activation': 'sigmoid'\n",
    "              }\n",
    "\n",
    "        model_layers = [layer1, layer2, layer3, layer4, layer5]\n",
    "\n",
    "        # optimization parameters\n",
    "        optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.0003,\n",
    "                    \"l2\": 1e-5,\n",
    "                    #\"label_smoothing\": 0.05,\n",
    "                    #\"l1\": 1e-6,\n",
    "                    }\n",
    "        return model_layers, optimization\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # get shapes of inputs and targets\n",
    "    input_shape = list(train['inputs'].shape)\n",
    "    input_shape[0] = None\n",
    "    output_shape = train['targets'].shape\n",
    "\n",
    "    # load model parameters\n",
    "    model_layers, optimization = cnn_model(input_shape, output_shape)\n",
    "\n",
    "    # build neural network class\n",
    "    nnmodel = nn.NeuralNet(seed=247)\n",
    "    nnmodel.build_layers(model_layers, optimization)\n",
    "\n",
    "    # compile neural trainer\n",
    "    save_path = os.path.join(params_results, exp)\n",
    "    param_path = os.path.join(save_path, modelsavename)\n",
    "    nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=param_path)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    sess = utils.initialize_session()\n",
    "    '''TEST'''\n",
    "    if TEST:\n",
    "\n",
    "        # set best parameters\n",
    "        nntrainer.set_best_parameters(sess)\n",
    "\n",
    "        # test model\n",
    "        loss, mean_vals, std_vals = nntrainer.test_model(sess, test, name='test')\n",
    "        if WRITE:\n",
    "            metricsline = '%s,%s,%s,%s,%s,%s,%s'%(exp, modelarch, trial, loss, mean_vals[0], mean_vals[1], mean_vals[2])\n",
    "            fd = open('test_metrics.csv', 'a')\n",
    "            fd.write(metricsline+'\\n')\n",
    "            fd.close()\n",
    "        '''SORT ACTIVATIONS'''\n",
    "        nntrainer.set_best_parameters(sess)\n",
    "        predictionsoutput = nntrainer.get_activations(sess, test, layer='output')\n",
    "        plot_index = np.argsort(predictionsoutput[:,0])[::-1]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "    '''FIRST ORDER MUTAGENESIS'''\n",
    "    if FOM:\n",
    "        num_plots = range(1)\n",
    "        for ii in num_plots: \n",
    "\n",
    "            X = np.expand_dims(test['inputs'][plot_index[ii]], axis=0)\n",
    "\n",
    "            mf.fom_heatmap(X, layer='dense_1_bias', alphabet='rna', nntrainer=nntrainer, sess=sess, figsize=(15,1.5))\n",
    "\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
