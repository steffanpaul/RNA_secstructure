{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "import helper \n",
    "from deepomics import neuralnetwork as nn\n",
    "from deepomics import utils, fit, visualize, saliency\n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE ACTIONS'''\n",
    "TRAIN = False\n",
    "TEST = False\n",
    "WRITE = False\n",
    "FOM = False\n",
    "SOMCALC = True\n",
    "SOMVIS = False\n",
    "\n",
    "if '--train' in sys.argv:\n",
    "  TRAIN = True\n",
    "if '--test' in sys.argv:\n",
    "  TEST = True\n",
    "if '--write' in sys.argv:\n",
    "  WRITE = True\n",
    "if '--fom' in sys.argv:\n",
    "  FOM = True\n",
    "if '--somcalc' in sys.argv:\n",
    "  SOMCALC = True\n",
    "if '--somvis' in sys.argv:\n",
    "  SOMVIS = True\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE LOOP'''\n",
    "trials = ['glna', 'trna', 'riboswitch']\n",
    "\n",
    "datafiles = {'glna': ['glna_100k_d8.hdf5', '../../data_RFAM/glnAsim_100k.sto'], \n",
    "              'trna': ['trna_100k_d4.hdf5', '../../data_RFAM/trnasim_100k.sto'],\n",
    "              'riboswitch': ['riboswitch_100k_d4.hdf5', '../../data_RFAM/riboswitch_100k.sto'],}\n",
    "\n",
    "exp = 'varfam'  #for both the data folder and the params folder\n",
    "exp_data = 'data_RFAM'\n",
    "\n",
    "img_folder = 'Imagesvar'\n",
    "\n",
    "for t in trials:\n",
    "\n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  '''OPEN DATA'''\n",
    "\n",
    "  starttime = time.time()\n",
    "\n",
    "  #Open data from h5py\n",
    "  filename = datafiles[t][0]\n",
    "  data_path = os.path.join('../..', exp_data, filename)\n",
    "  with h5py.File(data_path, 'r') as dataset:\n",
    "      X_data = np.array(dataset['X_data'])\n",
    "      Y_data = np.array(dataset['Y_data'])\n",
    "      \n",
    "  numdata, seqlen, _, dims = X_data.shape\n",
    "  dims = dims-1\n",
    "\n",
    "  #remove gaps from sequences\n",
    "  ungapped = True\n",
    "  if ungapped:\n",
    "      X_data = X_data[:, :, :, :dims]\n",
    "      \n",
    "  # get validation and test set from training set\n",
    "  test_frac = 0.3\n",
    "  valid_frac = 0.1\n",
    "  N = numdata\n",
    "  split_1 = int(N*(1-valid_frac-test_frac))\n",
    "  split_2 = int(N*(1-test_frac))\n",
    "  shuffle = np.random.permutation(N)\n",
    "\n",
    "  def unalign(X):\n",
    "    nuc_index = np.where(np.sum(X, axis=2)!=0)\n",
    "    return (X[nuc_index])\n",
    "\n",
    "  X_data_unalign = [unalign(X) for X in X_data]\n",
    "  maxlength = helper.get_maxlength(X_data_unalign)\n",
    "  X_data_unalign = np.expand_dims(helper.pad_inputs(X_data_unalign, MAX=maxlength)[0], axis=2)\n",
    "\n",
    "\n",
    "\n",
    "  #set up dictionaries\n",
    "  train = {'inputs': X_data_unalign[shuffle[:split_1]], \n",
    "           'targets': Y_data[shuffle[:split_1]]}\n",
    "  valid = {'inputs': X_data_unalign[shuffle[split_1:split_2]], \n",
    "           'targets': Y_data[shuffle[split_1:split_2]]}\n",
    "  test = {'inputs': X_data_unalign[shuffle[split_2:]], \n",
    "           'targets': Y_data[shuffle[split_2:]]}\n",
    "\n",
    "  #set up dictionaries\n",
    "  train_align = {'inputs': X_data[shuffle[:split_1]], \n",
    "           'targets': Y_data[shuffle[:split_1]]}\n",
    "  valid_align = {'inputs': X_data[shuffle[split_1:split_2]], \n",
    "           'targets': Y_data[shuffle[split_1:split_2]]}\n",
    "  test_align = {'inputs': X_data[shuffle[split_2:]], \n",
    "           'targets': Y_data[shuffle[split_2:]]}\n",
    "      \n",
    "  print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))\n",
    "\n",
    "  simalign_file = datafiles[t][1]\n",
    "  #Get the full secondary structure and sequence consensus from the emission\n",
    "  SS = mf.getSSconsensus(simalign_file)\n",
    "  SQ = mf.getSQconsensus(simalign_file)\n",
    "\n",
    "  #Get the ungapped sequence and the indices of ungapped nucleotides\n",
    "  _, ugSS, ugidx = mf.rm_consensus_gaps(X_data, SS)\n",
    "  _, ugSQ, _ = mf.rm_consensus_gaps(X_data, SQ)\n",
    "\n",
    "\n",
    "  #Get the sequence and indices of the conserved base pairs\n",
    "  bpchars = ['(',')','<','>','{','}']\n",
    "  sig_bpchars = ['<','>']\n",
    "  bpidx, bpSS, nonbpidx = mf.sigbasepair(SS, bpchars)\n",
    "  numbp = len(bpidx)\n",
    "  numug = len(ugidx)\n",
    "\n",
    "  #Get the bpug information\n",
    "  bpugSQ, bpugidx = mf.bpug(ugidx, bpidx, SQ)\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "  '''SAVE PATHS AND PARAMETERS'''\n",
    "  params_results = '../../results'\n",
    "\n",
    "  modelarch = 'resbind'\n",
    "  trial = t\n",
    "  modelsavename = '%s_%s'%(modelarch, trial)\n",
    "\n",
    "\n",
    "\n",
    "  '''BUILD NEURAL NETWORK'''\n",
    "\n",
    "  def cnn_model(input_shape, output_shape):\n",
    "\n",
    "      # create model\n",
    "      layer1 = {'layer': 'input', #41\n",
    "              'input_shape': input_shape\n",
    "              }\n",
    "      layer2 = {'layer': 'conv1d',\n",
    "              'num_filters': 96,\n",
    "              'filter_size': input_shape[1]-29,\n",
    "              'norm': 'batch',\n",
    "              'activation': 'relu',\n",
    "              'dropout': 0.3,\n",
    "              'padding': 'VALID',\n",
    "              }\n",
    "      layer3 = {'layer': 'conv1d_residual',\n",
    "              'filter_size': 5,\n",
    "              'function': 'relu',\n",
    "              'dropout_block': 0.1,\n",
    "              'dropout': 0.3,\n",
    "              'mean_pool': 10,\n",
    "              }\n",
    "      \n",
    "      layer4 = {'layer': 'dense',        # input, conv1d, dense, conv1d_residual, dense_residual, conv1d_transpose,\n",
    "                                      # concat, embedding, variational_normal, variational_softmax, + more\n",
    "            'num_units': 196,\n",
    "            'norm': 'batch',          # if removed, automatically adds bias instead\n",
    "            'activation': 'relu',     # or leaky_relu, prelu, sigmoid, tanh, etc\n",
    "            'dropout': 0.5,           # if removed, default is no dropout\n",
    "               }\n",
    "\n",
    "      \n",
    "      layer5 = {'layer': 'dense',\n",
    "              'num_units': output_shape[1],\n",
    "              'activation': 'sigmoid'\n",
    "              }\n",
    "\n",
    "      model_layers = [layer1, layer2, layer3, layer4, layer5]\n",
    "\n",
    "      # optimization parameters\n",
    "      optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.0003,\n",
    "                    \"l2\": 1e-5,\n",
    "                    #\"label_smoothing\": 0.05,\n",
    "                    #\"l1\": 1e-6,\n",
    "                    }\n",
    "      return model_layers, optimization\n",
    "\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # get shapes of inputs and targets\n",
    "  input_shape = list(train['inputs'].shape)\n",
    "  input_shape[0] = None\n",
    "  output_shape = train['targets'].shape\n",
    "\n",
    "  # load model parameters\n",
    "  model_layers, optimization = cnn_model(input_shape, output_shape)\n",
    "\n",
    "  # build neural network class\n",
    "  nnmodel = nn.NeuralNet(seed=247)\n",
    "  nnmodel.build_layers(model_layers, optimization)\n",
    "\n",
    "  # compile neural trainer\n",
    "  save_path = os.path.join(params_results, exp)\n",
    "  param_path = os.path.join(save_path, modelsavename)\n",
    "  nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=param_path)\n",
    "\n",
    "  \n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  '''TRAIN '''\n",
    "  if TRAIN:\n",
    "    # initialize session\n",
    "    sess = utils.initialize_session()\n",
    "\n",
    "    #Train the model\n",
    "\n",
    "    data = {'train': train, 'valid': valid}\n",
    "    fit.train_minibatch(sess, nntrainer, data, \n",
    "                      batch_size=100, \n",
    "                      num_epochs=100,\n",
    "                      patience=40, \n",
    "                      verbose=2, \n",
    "                      shuffle=True, \n",
    "                      save_all=False)\n",
    "\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------      \n",
    "  '''TEST'''\n",
    "  sess = utils.initialize_session()\n",
    "  if TEST:\n",
    "    \n",
    "    # set best parameters\n",
    "    nntrainer.set_best_parameters(sess)\n",
    "\n",
    "    # test model\n",
    "    loss, mean_vals, std_vals = nntrainer.test_model(sess, test, name='test')\n",
    "    if WRITE:\n",
    "      metricsline = '%s,%s,%s,%s,%s,%s,%s'%(exp, modelarch, trial, loss, mean_vals[0], mean_vals[1], mean_vals[2])\n",
    "      fd = open('test_metrics.csv', 'a')\n",
    "      fd.write(metricsline+'\\n')\n",
    "      fd.close()\n",
    "  '''SORT ACTIVATIONS'''\n",
    "  nntrainer.set_best_parameters(sess)\n",
    "  predictionsoutput = nntrainer.get_activations(sess, test, layer='output')\n",
    "  plot_index = np.argsort(predictionsoutput[:,0])[::-1]\n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "  '''FIRST ORDER MUTAGENESIS'''\n",
    "  if FOM:\n",
    "    plots = 3\n",
    "    num_plots = range(plots)\n",
    "    fig = plt.figure(figsize=(15,plots*2+1))\n",
    "    for ii in num_plots: \n",
    "\n",
    "        X = np.expand_dims(test['inputs'][plot_index[10000+ii]], axis=0)\n",
    "        \n",
    "        ax = fig.add_subplot(plots, 1, ii+1)\n",
    "        mf.fom_saliency_mul(X, layer='dense_1_bias', alphabet='rna', nntrainer=nntrainer, sess=sess, ax =ax)\n",
    "        fom_file = modelsavename + 'FoM' + '.png'\n",
    "    fom_file = os.path.join(img_folder, fom_file)\n",
    "    plt.savefig(fom_file)\n",
    "\n",
    "    plt.close()\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "  '''SECOND ORDER MUTAGENESIS'''\n",
    "\n",
    "  '''Som calc'''\n",
    "  if SOMCALC:\n",
    "    num_summary = 100\n",
    "\n",
    "    arrayspath = 'Arraysvar/%s_%s%s_so%.0fk.npy'%(exp, modelarch, trial, num_summary/1000)\n",
    "    X = test_align['inputs'][plot_index[1100:1100+num_summary]]\n",
    "\n",
    "    mean_mut2 = helper.som_average_ungapped_logodds_unalign(X, ugidx,maxlength, arrayspath, nntrainer, sess, progress='short', \n",
    "                                               save=True, layer='dense_1_bias')\n",
    "\n",
    "  if SOMVIS:  \n",
    "    #Load the saved data\n",
    "    num_summary = 2000\n",
    "    arrayspath = 'Arraysvar/%s_%s%s_so%.0fk.npy'%(exp, modelarch, trial, num_summary/1000)\n",
    "    mean_mut2 = np.load(arrayspath)\n",
    "\n",
    "    #Reshape into a holistic tensor organizing the mutations into 4*4\n",
    "    meanhol_mut2 = mean_mut2.reshape(numug,numug,4,4)\n",
    "\n",
    "    #Normalize\n",
    "    normalize = True\n",
    "    if normalize:\n",
    "        norm_meanhol_mut2 = mf.normalize_mut_hol(meanhol_mut2, nntrainer, sess, normfactor=1)\n",
    "\n",
    "    #Let's try something weird\n",
    "    bpfilter = np.ones((4,4))*0.\n",
    "    for i,j in zip(range(4), range(4)):\n",
    "        bpfilter[i, -(j+1)] = +1.\n",
    "\n",
    "    nofilter = np.ones((4,4))\n",
    "\n",
    "    C = (norm_meanhol_mut2*bpfilter)\n",
    "    C = np.sum((C).reshape(numug,numug,dims*dims), axis=2)\n",
    "    #C = C - np.mean(C)\n",
    "    #C = C/np.max(C)\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    sb.heatmap(C,vmin=None, cmap='Blues', linewidth=0.0)\n",
    "    plt.title('Base Pair scores: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "    som_file = modelsavename + 'SoM_bpfilter' + '.png'\n",
    "    som_file = os.path.join(img_folder, som_file)\n",
    "    plt.savefig(som_file)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    blocklen = np.sqrt(np.product(meanhol_mut2.shape)).astype(int)\n",
    "    S = np.zeros((blocklen, blocklen))\n",
    "    i,j,k,l = meanhol_mut2.shape\n",
    "\n",
    "    for ii in range(i):\n",
    "        for jj in range(j):\n",
    "            for kk in range(k):\n",
    "                for ll in range(l):\n",
    "                    S[(4*ii)+kk, (4*jj)+ll] = meanhol_mut2[ii,jj,kk,ll]\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(S,  cmap='Reds', vmin=None)\n",
    "    plt.colorbar()\n",
    "    plt.title('Blockvis of all mutations: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "    som_file = modelsavename + 'SoM_blockvis' + '.png'\n",
    "    som_file = os.path.join(img_folder, som_file)\n",
    "    plt.savefig(som_file)\n",
    "    plt.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
