{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable\n",
    "## Is it just size that is breaking them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and dict construction completed in: 1.31s\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/rnn_var10med_best\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/rnn_var10med_best\n",
      "[=========================] 99.6% -- elapsed time=7.43s -- loss=0.12167 -- acc=0.0000000\n",
      "\n",
      "  test loss  = 0.12166587124638101\n",
      "  test acc   = 0.9575666666666667\n",
      "  test AUROC = 0.9887455359516918\n",
      "  test AUPRC = 0.9846890027131621\n",
      "[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "[8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48]\n",
      "[7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]\n",
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43]\n",
      "[9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "import helper \n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd\n",
    "\n",
    "def make_variable_length(X_train_fixed, addon=5):\n",
    "    N = len(X_train_fixed)\n",
    "    start_buffer = np.random.randint(1, addon, size=N)\n",
    "    end_buffer = np.random.randint(1, addon, size=N)\n",
    "\n",
    "    X_train_all = []\n",
    "    starts = []\n",
    "    for n in range(N):\n",
    "        start_size = start_buffer[n]\n",
    "        start_index = np.random.randint(0,4, size=start_size)\n",
    "        start_one_hot = np.zeros((start_size, 4))\n",
    "        for i in range(start_size):\n",
    "            start_one_hot[i, start_index[i]] = 1\n",
    "        starts.append(len(start_index))\n",
    "        \n",
    "        end_size = end_buffer[n]\n",
    "        end_index = np.random.randint(0,4, size=end_size)\n",
    "        end_one_hot = np.zeros((end_size, 4))\n",
    "        for i in range(end_size):\n",
    "            end_one_hot[i, end_index[i]] = 1\n",
    "        X_train_all.append(np.vstack([start_one_hot, X_train_fixed[n], end_one_hot]))\n",
    "    return X_train_all, starts\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''DEFINE ACTIONS'''\n",
    "TRAIN = False\n",
    "TEST = True\n",
    "WRITE = False\n",
    "FOM = False\n",
    "SOMCALC = True\n",
    "SOMVIS = False\n",
    "\n",
    "\n",
    "\n",
    "'''DEFINE LOOP'''\n",
    "trials = ['med']#['small', 'med', 'large']\n",
    "varlengths = [10]#, 30] #20\n",
    "exp = 'toyhp'  #for both the data folder and the params folder\n",
    "exp_data = 'data_%s'%(exp)\n",
    "\n",
    "img_folder = 'Images'\n",
    "\n",
    "for t in trials:\n",
    "    for v in varlengths:\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        '''OPEN DATA'''\n",
    "\n",
    "        starttime = time.time()\n",
    "\n",
    "        #Open data from h5py\n",
    "        \n",
    "        filename = '%s_50k_%s.hdf5'%(exp, t)\n",
    "        data_path = os.path.join('../..', exp_data, filename)\n",
    "        with h5py.File(data_path, 'r') as dataset:\n",
    "            X_data = np.array(dataset['X_data'])\n",
    "            Y_data = np.array(dataset['Y_data'])\n",
    "        \n",
    "        numdata, seqlen, dims = X_data.shape\n",
    "\n",
    "        #Make variable!\n",
    "        addon = v\n",
    "        X_data, starts = make_variable_length(X_data, addon=addon)\n",
    "\n",
    "        # get validation and test set from training set\n",
    "        test_frac = 0.3\n",
    "        valid_frac = 0.1\n",
    "        N = numdata\n",
    "        split_1 = int(N*(1-valid_frac-test_frac))\n",
    "        split_2 = int(N*(1-test_frac))\n",
    "        shuffle = np.random.permutation(N)\n",
    "\n",
    "        X_train = [X_data[s] for s in shuffle[:split_1]]\n",
    "        X_valid = [X_data[s] for s in shuffle[split_1:split_2]]\n",
    "        X_test = [X_data[s] for s in shuffle[split_2:]]\n",
    "        test_starts = np.asarray(starts)[shuffle[split_2:]]\n",
    "\n",
    "        Y_train = Y_data[shuffle[:split_1]]\n",
    "        Y_valid = Y_data[shuffle[split_1:split_2]]\n",
    "        Y_test = Y_data[shuffle[split_2:]]\n",
    "          \n",
    "        print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        '''BUILD MODEL AND OPTIMIZER'''\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        num_hidden = 64\n",
    "        num_layers = 2\n",
    "        num_classes = Y_train.shape[1]\n",
    "\n",
    "        # tf Graph input\n",
    "        X = tf.placeholder(tf.float32, [None, None, X_train[0].shape[1]], name='inputs')\n",
    "        Y = tf.placeholder(tf.float32, [None, num_classes], name='ouputs')\n",
    "        keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "        lstm1_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "        lstm1_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm1_fw_cell, \n",
    "                                                 output_keep_prob=keep_prob, \n",
    "                                                 state_keep_prob=1.0,\n",
    "                                                 variational_recurrent=False,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        lstm1_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "        lstm1_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm1_bw_cell, \n",
    "                                                 output_keep_prob=keep_prob, \n",
    "                                                 state_keep_prob=1.0,\n",
    "                                                 variational_recurrent=False,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(lstm1_fw_cell, lstm1_bw_cell, X, \n",
    "                                                           sequence_length=helper.length(X), dtype=tf.float32,\n",
    "                                                           scope='BLSTM_1')\n",
    "\n",
    "        outputs_forward, outputs_backward = outputs1\n",
    "\n",
    "        # states_forward is a tuple of (c is the hidden state and h is the output)\n",
    "        concat_outputs = tf.concat([outputs_forward, outputs_backward], axis=2, name='intermediate')\n",
    "\n",
    "        lstm2_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "        lstm2_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm2_fw_cell, \n",
    "                                                 output_keep_prob=keep_prob, \n",
    "                                                 state_keep_prob=1.0,\n",
    "                                                 variational_recurrent=False,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        lstm2_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "        lstm2_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm2_bw_cell, \n",
    "                                                 output_keep_prob=keep_prob, \n",
    "                                                 state_keep_prob=1.0,\n",
    "                                                 variational_recurrent=False,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        outputs2, states2 = tf.nn.bidirectional_dynamic_rnn(lstm2_fw_cell, lstm2_bw_cell, concat_outputs,\n",
    "                                                            scope='BLSTM_2', dtype=tf.float32)\n",
    "\n",
    "        states_forward, states_backward = states2\n",
    "\n",
    "        # states_forward is a tuple of (c is the hidden state and h is the output)\n",
    "        concat_states = tf.concat([states_forward[1], states_backward[1]], axis=1, name='output')\n",
    "\n",
    "        # Linear activation, using rnn inner loop last output\n",
    "        W_out = tf.Variable(tf.random_normal([num_hidden*2, num_classes]))\n",
    "        b_out = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "        #last = tf.gather(outputs, int(outputs.get_shape()[1])-1)  \n",
    "        #last = int(outputs.get_shape()[1]) - 1\n",
    "        logits = tf.matmul(concat_states, W_out) + b_out\n",
    "        predictions = tf.nn.sigmoid(logits)\n",
    "\n",
    "        # The Optimizer\n",
    "\n",
    "        learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "        # Define loss and optimizer\n",
    "        predictions = tf.clip_by_value(predictions, clip_value_max=1-1e-7, clip_value_min=1e-7)\n",
    "        #cost = tf.reduce_sum(Y*tf.log(predictions), axis=1)\n",
    "        cost = tf.reduce_sum(Y*tf.log(predictions)+(1-Y)*tf.log(1-predictions), axis=1)\n",
    "\n",
    "        total_loss = tf.reduce_mean(-cost)\n",
    "\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "        grads = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "        # Apply gradients.\n",
    "        apply_gradient_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "        # Track the moving averages of all trainable variables.\n",
    "        variable_averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "        variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "        with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "            train_op = tf.no_op(name='train')\n",
    "\n",
    "        # Evaluate model (with test logits, for dropout to be disabled)\n",
    "        #correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(Y, 1))\n",
    "        correct_pred = tf.equal(tf.round(predictions), Y)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        '''SAVE PATHS AND PARAMETERS'''\n",
    "        params_results = '../../results'\n",
    "\n",
    "        modelarch = 'rnn'\n",
    "        trial = 'var' + str(v) + t\n",
    "        modelsavename = '%s_%s'%(modelarch, trial)\n",
    "\n",
    "        '''LOAD PARAMETERS'''\n",
    "        save_path = os.path.join(params_results, exp)\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "            print(\"making directory: \" + save_path)\n",
    "        params_filename = '%s_%s_best'%(modelarch, trial)\n",
    "        params_path = os.path.join(save_path, params_filename)\n",
    "\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # restore trained parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path=params_path)\n",
    "\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "        '''TRAIN MODEL'''\n",
    "        if TRAIN:\n",
    "            \n",
    "            sess = tf.Session()\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            # restore trained parameters\n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(sess, save_path=params_path)\n",
    "            \n",
    "\n",
    "            batch_size = 128\n",
    "            train_batches = helper.bucket_generator(X_train, Y_train, batch_size)\n",
    "            valid_batches = helper.bucket_generator(X_valid, Y_valid, batch_size)\n",
    "            num_epochs = 50\n",
    "            bar_length = 25\n",
    "            patience = 10\n",
    "\n",
    "\n",
    "            # path to save results\n",
    "            save_path = os.path.join(params_results, exp)\n",
    "            if not os.path.isdir(save_path):\n",
    "                os.mkdir(save_path)\n",
    "                print(\"making directory: \" + save_path)\n",
    "            params_filename = '%s_%s_best'%(modelarch, trial)\n",
    "            params_path = os.path.join(save_path, params_filename)\n",
    "\n",
    "            wait=0\n",
    "            min_loss = 1e10\n",
    "            for epoch in range(num_epochs):\n",
    "                print('epoch: '+ str(epoch+1))\n",
    "\n",
    "                num_batches = len(train_batches)\n",
    "                shuffled_batches = []\n",
    "                for i in np.random.permutation(num_batches):\n",
    "                    shuffled_batches.append(train_batches[i])\n",
    "\n",
    "                loss = 0\n",
    "                acc = 0\n",
    "                start_time = time.time()\n",
    "                for i, batch in enumerate(shuffled_batches):\n",
    "                    batch_loss, batch_acc, _ = sess.run([total_loss, accuracy, train_op], feed_dict={X: batch[0], \n",
    "                                                                                                  Y: batch[1], \n",
    "                                                                                                  keep_prob: 0.5,\n",
    "                                                                                                  learning_rate: 0.0003})            \n",
    "                    loss += batch_loss\n",
    "                    acc += batch_acc\n",
    "\n",
    "                    remaining_time = (time.time()-start_time)*(num_batches-(i+1))/(i+1)\n",
    "                    percent = float(i)/num_batches\n",
    "                    progress = '='*int(round(percent*bar_length))\n",
    "                    spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "                    sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs -- loss=%.5f -- acc=%.5f\" \\\n",
    "                    %(progress+spaces, percent*100, remaining_time, loss/(i+1), acc/(i+1)))\n",
    "\n",
    "                sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs -- loss=%.5f -- acc=%.5f\\n\" \\\n",
    "                %(progress+spaces, percent*100, time.time()-start_time, loss/(i+1), acc/(i+1)))\n",
    "                sys.stdout.write(\"\\n\")\n",
    "\n",
    "\n",
    "                num_batches = len(valid_batches)\n",
    "                loss = 0\n",
    "                acc = 0\n",
    "                valid_predictions = []\n",
    "                valid_truth = []\n",
    "                start_time = time.time()\n",
    "                for i, batch in enumerate(valid_batches):\n",
    "                    batch_loss, batch_predict = sess.run([total_loss, predictions], feed_dict={X: batch[0], \n",
    "                                                                                            Y: batch[1], \n",
    "                                                                                            keep_prob: 1.0})            \n",
    "                    loss += batch_loss\n",
    "                    valid_predictions.append(batch_predict)\n",
    "                    valid_truth.append(batch[1])\n",
    "                valid_loss = loss/num_batches\n",
    "                valid_predictions = np.vstack(valid_predictions)\n",
    "                valid_truth = np.vstack(valid_truth)\n",
    "\n",
    "                correct = np.mean(np.equal(valid_truth, np.round(valid_predictions)))\n",
    "                auc_roc, roc_curves = helper.roc(valid_truth, valid_predictions)\n",
    "                auc_pr, pr_curves = helper.pr(valid_truth, valid_predictions)\n",
    "                print(\"  valid loss  = \"+str(loss/num_batches))\n",
    "                print(\"  valid acc   = \"+str(np.nanmean(correct)))\n",
    "                print(\"  valid AUROC = \"+str(np.nanmean(auc_roc)))\n",
    "                print(\"  valid AUPRC = \"+str(np.nanmean(auc_pr)))\n",
    "\n",
    "                # check if current validation loss is lower, if so, save parameters, if not check patience\n",
    "                if valid_loss < min_loss:\n",
    "                    print(\"  Lower validation loss found. Saving parameters to: \"+params_path)\n",
    "\n",
    "                    # save model parameters\n",
    "                    saver = tf.train.Saver()\n",
    "                    saver.save(sess, save_path=params_path)\n",
    "\n",
    "                    # set minimum loss to the current validation loss\n",
    "                    min_loss = valid_loss\n",
    "\n",
    "                    # reset wait time\n",
    "                    wait = 0\n",
    "                else:\n",
    "\n",
    "                    # add to wait time\n",
    "                    wait += 1\n",
    "\n",
    "                    # check to see if patience has run out\n",
    "                    if wait == patience:\n",
    "                        print(\"Patience ran out... early stopping!\")\n",
    "                        break\n",
    "\n",
    "            # close tensorflow session (Note, the graph is still open)\n",
    "            sess.close()\n",
    "        \n",
    "        \n",
    "#---------------------------------------------------------------------------------------------\n",
    "        '''TEST'''\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        # restore trained parameters\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess, save_path=params_path)\n",
    "\n",
    "        batch_size = 128\n",
    "        batches, sort_index = helper.bucket_generator(X_test, Y_test, batch_size, index=True)\n",
    "        num_batches = len(batches)\n",
    "\n",
    "        loss = 0\n",
    "        acc = 0\n",
    "        valid_predictions = []\n",
    "        valid_truth = []\n",
    "        start_time = time.time()\n",
    "        num_batches = len(batches)\n",
    "        bar_length = 25\n",
    "\n",
    "        for i, batch in enumerate(batches):\n",
    "\n",
    "            batch_loss, batch_predict, batch_logits = sess.run([total_loss, predictions, logits], feed_dict={X: batch[0], \n",
    "                                                                                    Y: batch[1], \n",
    "                                                                                    keep_prob: 1.0})            \n",
    "            loss += batch_loss\n",
    "            valid_predictions.append(batch_predict)\n",
    "            valid_truth.append(batch[1])\n",
    "            \n",
    "            remaining_time = (time.time()-start_time)*(num_batches-(i+1))/(i+1)\n",
    "            percent = float(i)/num_batches\n",
    "            progress = '='*int(round(percent*bar_length))\n",
    "            spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "            sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs -- loss=%.5f -- acc=%.5f\" \\\n",
    "            %(progress+spaces, percent*100, remaining_time, loss/(i+1), acc/(i+1)))\n",
    "\n",
    "        sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs -- loss=%.5f -- acc=%.5f\\n\" \\\n",
    "        %(progress+spaces, percent*100, time.time()-start_time, loss/(i+1), acc/(i+1)))\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "        valid_predictions = np.vstack(valid_predictions)\n",
    "        valid_truth = np.vstack(valid_truth)\n",
    "\n",
    "        correct = np.mean(np.equal(valid_truth, np.round(valid_predictions)))\n",
    "        auc_roc, roc_curves = helper.roc(valid_truth, valid_predictions)\n",
    "        auc_pr, pr_curves = helper.pr(valid_truth, valid_predictions)\n",
    "        mean_vals = [np.nanmean(correct), np.nanmean(auc_roc), np.nanmean(auc_pr)]\n",
    "        std = [np.nanstd(correct), np.nanstd(auc_roc), np.nanstd(auc_pr)]\n",
    "\n",
    "        print(\"  test loss  = \"+str(loss/num_batches))\n",
    "        print(\"  test acc   = \"+str(np.nanmean(correct)))\n",
    "        print(\"  test AUROC = \"+str(np.nanmean(auc_roc)))\n",
    "        print(\"  test AUPRC = \"+str(np.nanmean(auc_pr)))\n",
    "        \n",
    "        if WRITE:\n",
    "            metricsline = '%s,%s,%s,%s,%s,%s,%s'%(exp, modelarch, trial, loss, mean_vals[0], mean_vals[1], mean_vals[2])\n",
    "            fd = open('test_metrics.csv', 'a')\n",
    "            fd.write(metricsline+'\\n')\n",
    "            fd.close()\n",
    "\n",
    "\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        '''SORT ACTIVATIONS'''\n",
    "        WT_predictions = valid_predictions[np.argsort(sort_index)]\n",
    "        plot_index = np.argsort(WT_predictions[:,0])[::-1]\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        '''SECOND ORDER MUTAGENESIS'''\n",
    "\n",
    "        '''Som Calc'''\n",
    "        if SOMCALC:\n",
    "            def second_order_mutagenesis(sess, predictions, X_val, ugidx):\n",
    "                seqlen, dims = X_val.shape\n",
    "                idxlen = len(ugidx)\n",
    "\n",
    "                # get wild-type score\n",
    "                wt_score = sess.run(predictions, feed_dict={X: np.expand_dims(X_val, axis=0), keep_prob: 1.0})[0]     \n",
    "\n",
    "                # generate mutagenesis sequences\n",
    "                num_mut = idxlen**2*dims**2\n",
    "                X_mut = np.einsum('nl,lka->nka', np.ones((num_mut, 1)), np.expand_dims(X_val, axis=0))\n",
    "\n",
    "                k=0\n",
    "                for position1 in ugidx:\n",
    "                    for position2 in ugidx:\n",
    "                        for nuc1 in range(dims):\n",
    "                            for nuc2 in range(dims):\n",
    "                                X_mut[k, position1, :] = 0\n",
    "                                X_mut[k, position1, nuc1] = 1        \n",
    "                                X_mut[k, position2, :] = 0\n",
    "                                X_mut[k, position2, nuc2] = 1        \n",
    "                                k += 1\n",
    "\n",
    "                # get second order mutagenesis score\n",
    "                X_mut = [x for x in X_mut]\n",
    "                mut_scores = []\n",
    "                batches = helper.batch_generator(X_mut, batch_size=512, MAX=None, shuffle_data=False)\n",
    "                for i, batch in enumerate(batches):\n",
    "                    batch_predict = sess.run(predictions, feed_dict={X: batch, keep_prob: 1.0})            \n",
    "                    mut_scores.append(batch_predict)\n",
    "                mut_scores = np.vstack(mut_scores)\n",
    "                \n",
    "                # calculate log-odds score\n",
    "                log_odds = np.log(mut_scores + 1e-7) - np.log(wt_score + 1e-7)\n",
    "\n",
    "\n",
    "                # reshape second order scores\n",
    "                second_mutagenesis_logodds = np.zeros((idxlen, idxlen, dims, dims))\n",
    "                k = 0\n",
    "                for i in range(idxlen):\n",
    "                    for j in range(idxlen):\n",
    "                        for m in range(dims):\n",
    "                            for n in range(dims):\n",
    "                                second_mutagenesis_logodds[i,j,m,n] = log_odds[k,0]\n",
    "                                k += 1\n",
    "                return second_mutagenesis_logodds\n",
    "\n",
    "\n",
    "            start_time = time.time()\n",
    "            np.random.seed(274)\n",
    "\n",
    "            bar_length = 50\n",
    "            N = 5\n",
    "            mutagenesis_logodds = []\n",
    "            for i, index in enumerate(plot_index[:N]):\n",
    "                ugidx = range(test_starts[index], seqlen + test_starts[index])\n",
    "                print (ugidx)\n",
    "                '''\n",
    "                logresult = second_order_mutagenesis(sess, logits, X_test[index], ugidx)\n",
    "                mutagenesis_logodds.append(logresult)\n",
    "\n",
    "\n",
    "                remaining_time = (time.time()-start_time)*(N-(i+1))/(i+1)\n",
    "                percent = float(i)/N\n",
    "                progress = '='*int(round(percent*bar_length))\n",
    "                spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "                sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs\" \\\n",
    "                %(progress+spaces, percent*100, remaining_time))\n",
    "\n",
    "            sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs\" \\\n",
    "            %(progress+spaces, percent*100, time.time()-start_time))\n",
    "            sys.stdout.write(\"\\n\")\n",
    "\n",
    "            mean_mut2 = np.nanmean(mutagenesis_logodds, axis=0)\n",
    "            idx = np.where(np.isnan(mean_mut2))\n",
    "            mean_mut2[idx] = np.min(mean_mut2)\n",
    "\n",
    "            arrayspath = 'Arrays/%s_%s%s_so.npy'%(exp, modelarch, trial)\n",
    "            np.save(arrayspath, mean_mut2)'''\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------\n",
    "        '''Som Vis'''\n",
    "\n",
    "        if SOMVIS:\n",
    "            arrayspath = 'Arrays/%s_%s%s_so.npy'%(exp, modelarch, trial)\n",
    "\n",
    "            mean_mut2 = np.load(arrayspath)\n",
    "\n",
    "            norm_mean_mut2 = helper.normalize_mut_hol(mean_mut2, normfactor=1)\n",
    "\n",
    "            #Let's try something weird\n",
    "            bpfilter = np.ones((4,4))*-1\n",
    "            for i,j in zip(range(4), range(4)):\n",
    "                bpfilter[i, -(j+1)] = 1.\n",
    "\n",
    "            C = np.sum((norm_mean_mut2*bpfilter).reshape(seqlen,seqlen,dims*dims), axis=2)\n",
    "            #C = C - np.mean(C)\n",
    "            #C = C/np.max(C)\n",
    "\n",
    "            color = 'Reds'\n",
    "\n",
    "            plt.figure(figsize=(8,6))\n",
    "            sb.heatmap(C, vmin=None, cmap=color , linewidth=0.00)\n",
    "            plt.title('Base Pair scores: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "            som_file = modelsavename + 'SoM_bpfilter' + '.png'\n",
    "            som_file = os.path.join(img_folder, som_file)\n",
    "            plt.savefig(som_file)\n",
    "            plt.close()\n",
    "\n",
    "            blocklen = np.sqrt(np.product(mean_mut2.shape)).astype(int)\n",
    "            S = np.zeros((blocklen, blocklen))\n",
    "            i,j,k,l = mean_mut2.shape\n",
    "\n",
    "            for ii in range(i):\n",
    "                for jj in range(j):\n",
    "                    for kk in range(k):\n",
    "                        for ll in range(l):\n",
    "                            S[(4*ii)+kk, (4*jj)+ll] = mean_mut2[ii,jj,kk,ll]\n",
    "\n",
    "            plt.figure(figsize=(15,15))\n",
    "            plt.imshow(S,  cmap='RdPu')\n",
    "            plt.colorbar()\n",
    "            plt.title('Blockvis of all mutations: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "            som_file =modelsavename+ 'SoM_blockvis' + '.png'\n",
    "            som_file = os.path.join(img_folder, som_file)\n",
    "            plt.savefig(som_file)\n",
    "            plt.close()\n",
    "        #---------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uniform\n",
    "## Is it just size that is breaking them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and dict construction completed in: 0.23s\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/rnn_small_best\n",
      "[=========================] 99.6% -- elapsed time=4.22s -- loss=0.01027 -- acc=0.0000000\n",
      "\n",
      "  test loss  = 0.01027090435059979\n",
      "  test acc   = 0.9974666666666666\n",
      "  test AUROC = 0.9994842039951288\n",
      "  test AUPRC = 0.999208559008914\n",
      "Data extraction and dict construction completed in: 0.2s\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/rnn_med_best\n",
      "[=========================] 99.6% -- elapsed time=5.45s -- loss=0.00352 -- acc=0.0000000\n",
      "\n",
      "  test loss  = 0.003524120594932143\n",
      "  test acc   = 0.9989333333333333\n",
      "  test AUROC = 0.999967350672159\n",
      "  test AUPRC = 0.999963844645924\n",
      "Data extraction and dict construction completed in: 0.22s\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/rnn_mod_best\n",
      "[=========================] 99.6% -- elapsed time=4.59s -- loss=0.03945 -- acc=0.0000000\n",
      "\n",
      "  test loss  = 0.03945264379751492\n",
      "  test acc   = 0.9877\n",
      "  test AUROC = 0.9977406398393345\n",
      "  test AUPRC = 0.996457784158634\n",
      "Data extraction and dict construction completed in: 0.39s\n",
      "INFO:tensorflow:Restoring parameters from ../../results/toyhp/rnn_large_best\n",
      "[=========================] 99.6% -- elapsed time=10.61s -- loss=0.69326 -- acc=0.000000\n",
      "\n",
      "  test loss  = 0.6932561953017052\n",
      "  test acc   = 0.5011666666666666\n",
      "  test AUROC = 0.499635532139055\n",
      "  test AUPRC = 0.4969730078306015\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "import helper \n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE ACTIONS'''\n",
    "TRAIN = False\n",
    "WRITE = False\n",
    "FOM = False\n",
    "SOMCALC = False\n",
    "SOMVIS = False\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE LOOP'''\n",
    "trials = ['small', 'med', 'mod', 'large']\n",
    "exp = 'toyhp'  #for both the data folder and the params folder\n",
    "exp_data = 'data_%s'%(exp)\n",
    "\n",
    "img_folder = 'Images'\n",
    "\n",
    "for t in trials:\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    '''OPEN DATA'''\n",
    "\n",
    "    starttime = time.time()\n",
    "\n",
    "    #Open data from h5py\n",
    "    filename = '%s_50k_%s.hdf5'%(exp, t)\n",
    "    data_path = os.path.join('../..', exp_data, filename)\n",
    "    with h5py.File(data_path, 'r') as dataset:\n",
    "        X_data = np.array(dataset['X_data'])\n",
    "        Y_data = np.array(dataset['Y_data'])\n",
    "      \n",
    "    numdata, seqlen, dims = X_data.shape\n",
    "    X_data = np.expand_dims(X_data, axis=2)\n",
    "      \n",
    "    # get validation and test set from training set\n",
    "    test_frac = 0.3\n",
    "    valid_frac = 0.1\n",
    "    N = numdata\n",
    "    split_1 = int(N*(1-valid_frac-test_frac))\n",
    "    split_2 = int(N*(1-test_frac))\n",
    "    shuffle = np.random.permutation(N)\n",
    "\n",
    "    X_train = X_data[shuffle[:split_1], :, 0, :]\n",
    "    X_valid = X_data[shuffle[split_1:split_2], :, 0, :]\n",
    "    X_test = X_data[shuffle[split_2:], :, 0, :]\n",
    "\n",
    "    Y_train = Y_data[shuffle[:split_1]]\n",
    "    Y_valid = Y_data[shuffle[split_1:split_2]]\n",
    "    Y_test = Y_data[shuffle[split_2:]]\n",
    "      \n",
    "    print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "    '''BUILD MODEL AND OPTIMIZER'''\n",
    "\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    num_hidden = 64\n",
    "    num_layers = 2\n",
    "    num_classes = Y_train.shape[1]\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None, None, X_train[0].shape[1]], name='inputs')\n",
    "    Y = tf.placeholder(tf.float32, [None, num_classes], name='ouputs')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    lstm1_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "    lstm1_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm1_fw_cell, \n",
    "                                             output_keep_prob=keep_prob, \n",
    "                                             state_keep_prob=1.0,\n",
    "                                             variational_recurrent=False,\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "    lstm1_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "    lstm1_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm1_bw_cell, \n",
    "                                             output_keep_prob=keep_prob, \n",
    "                                             state_keep_prob=1.0,\n",
    "                                             variational_recurrent=False,\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "    outputs1, states1 = tf.nn.bidirectional_dynamic_rnn(lstm1_fw_cell, lstm1_bw_cell, X, \n",
    "                                                       sequence_length=helper.length(X), dtype=tf.float32,\n",
    "                                                       scope='BLSTM_1')\n",
    "\n",
    "    outputs_forward, outputs_backward = outputs1\n",
    "\n",
    "    # states_forward is a tuple of (c is the hidden state and h is the output)\n",
    "    concat_outputs = tf.concat([outputs_forward, outputs_backward], axis=2, name='intermediate')\n",
    "\n",
    "    lstm2_fw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "    lstm2_fw_cell = tf.contrib.rnn.DropoutWrapper(lstm2_fw_cell, \n",
    "                                             output_keep_prob=keep_prob, \n",
    "                                             state_keep_prob=1.0,\n",
    "                                             variational_recurrent=False,\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "    lstm2_bw_cell = tf.nn.rnn_cell.LSTMCell(num_hidden)#, forget_bias=1.0)\n",
    "    lstm2_bw_cell = tf.contrib.rnn.DropoutWrapper(lstm2_bw_cell, \n",
    "                                             output_keep_prob=keep_prob, \n",
    "                                             state_keep_prob=1.0,\n",
    "                                             variational_recurrent=False,\n",
    "                                             dtype=tf.float32)\n",
    "\n",
    "    outputs2, states2 = tf.nn.bidirectional_dynamic_rnn(lstm2_fw_cell, lstm2_bw_cell, concat_outputs,\n",
    "                                                        scope='BLSTM_2', dtype=tf.float32)\n",
    "\n",
    "    states_forward, states_backward = states2\n",
    "\n",
    "    # states_forward is a tuple of (c is the hidden state and h is the output)\n",
    "    concat_states = tf.concat([states_forward[1], states_backward[1]], axis=1, name='output')\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    W_out = tf.Variable(tf.random_normal([num_hidden*2, num_classes]))\n",
    "    b_out = tf.Variable(tf.random_normal([num_classes]))\n",
    "\n",
    "    #last = tf.gather(outputs, int(outputs.get_shape()[1])-1)  \n",
    "    #last = int(outputs.get_shape()[1]) - 1\n",
    "    logits = tf.matmul(concat_states, W_out) + b_out\n",
    "    predictions = tf.nn.sigmoid(logits)\n",
    "\n",
    "    # The Optimizer\n",
    "\n",
    "    learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    predictions = tf.clip_by_value(predictions, clip_value_max=1-1e-7, clip_value_min=1e-7)\n",
    "    #cost = tf.reduce_sum(Y*tf.log(predictions), axis=1)\n",
    "    cost = tf.reduce_sum(Y*tf.log(predictions)+(1-Y)*tf.log(1-predictions), axis=1)\n",
    "\n",
    "    total_loss = tf.reduce_mean(-cost)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    grads = optimizer.compute_gradients(total_loss)\n",
    "\n",
    "    # Apply gradients.\n",
    "    apply_gradient_op = optimizer.apply_gradients(grads)\n",
    "\n",
    "    # Track the moving averages of all trainable variables.\n",
    "    variable_averages = tf.train.ExponentialMovingAverage(0.9)\n",
    "    variables_averages_op = variable_averages.apply(tf.trainable_variables())\n",
    "\n",
    "    with tf.control_dependencies([apply_gradient_op, variables_averages_op]):\n",
    "        train_op = tf.no_op(name='train')\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    #correct_pred = tf.equal(tf.argmax(predictions, 1), tf.argmax(Y, 1))\n",
    "    correct_pred = tf.equal(tf.round(predictions), Y)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "    '''SAVE PATHS AND PARAMETERS'''\n",
    "    params_results = '../../results'\n",
    "\n",
    "    modelarch = 'rnn'\n",
    "    trial = t\n",
    "    modelsavename = '%s_%s'%(modelarch, trial)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    \n",
    "    '''TRAIN MODEL'''\n",
    "    if TRAIN:\n",
    "        \n",
    "        # start session\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        batch_size = 128\n",
    "        train_batches = helper.bucket_generator(X_train, Y_train, batch_size)\n",
    "        valid_batches = helper.bucket_generator(X_valid, Y_valid, batch_size)\n",
    "        num_epochs = 50\n",
    "        bar_length = 25\n",
    "        patience = 10\n",
    "\n",
    "\n",
    "        # path to save results\n",
    "        save_path = os.path.join(params_results, exp)\n",
    "        if not os.path.isdir(save_path):\n",
    "            os.mkdir(save_path)\n",
    "            print(\"making directory: \" + save_path)\n",
    "        params_filename = '%s_%s_best'%(modelarch, trial)\n",
    "        params_path = os.path.join(save_path, params_filename)\n",
    "\n",
    "        wait=0\n",
    "        min_loss = 1e10\n",
    "        for epoch in range(num_epochs):\n",
    "            print('epoch: '+ str(epoch+1))\n",
    "\n",
    "            num_batches = len(train_batches)\n",
    "            shuffled_batches = []\n",
    "            for i in np.random.permutation(num_batches):\n",
    "                shuffled_batches.append(train_batches[i])\n",
    "\n",
    "            loss = 0\n",
    "            acc = 0\n",
    "            start_time = time.time()\n",
    "            for i, batch in enumerate(shuffled_batches):\n",
    "                batch_loss, batch_acc, _ = sess.run([total_loss, accuracy, train_op], feed_dict={X: batch[0], \n",
    "                                                                                              Y: batch[1], \n",
    "                                                                                              keep_prob: 0.5,\n",
    "                                                                                              learning_rate: 0.0003})            \n",
    "                loss += batch_loss\n",
    "                acc += batch_acc\n",
    "\n",
    "                remaining_time = (time.time()-start_time)*(num_batches-(i+1))/(i+1)\n",
    "                percent = float(i)/num_batches\n",
    "                progress = '='*int(round(percent*bar_length))\n",
    "                spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "                sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs -- loss=%.5f -- acc=%.5f\" \\\n",
    "                %(progress+spaces, percent*100, remaining_time, loss/(i+1), acc/(i+1)))\n",
    "\n",
    "            sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs -- loss=%.5f -- acc=%.5f\\n\" \\\n",
    "            %(progress+spaces, percent*100, time.time()-start_time, loss/(i+1), acc/(i+1)))\n",
    "            sys.stdout.write(\"\\n\")\n",
    "\n",
    "\n",
    "            num_batches = len(valid_batches)\n",
    "            loss = 0\n",
    "            acc = 0\n",
    "            valid_predictions = []\n",
    "            valid_truth = []\n",
    "            start_time = time.time()\n",
    "            for i, batch in enumerate(valid_batches):\n",
    "                batch_loss, batch_predict = sess.run([total_loss, predictions], feed_dict={X: batch[0], \n",
    "                                                                                        Y: batch[1], \n",
    "                                                                                        keep_prob: 1.0})            \n",
    "                loss += batch_loss\n",
    "                valid_predictions.append(batch_predict)\n",
    "                valid_truth.append(batch[1])\n",
    "            valid_loss = loss/num_batches\n",
    "            valid_predictions = np.vstack(valid_predictions)\n",
    "            valid_truth = np.vstack(valid_truth)\n",
    "\n",
    "            correct = np.mean(np.equal(valid_truth, np.round(valid_predictions)))\n",
    "            auc_roc, roc_curves = helper.roc(valid_truth, valid_predictions)\n",
    "            auc_pr, pr_curves = helper.pr(valid_truth, valid_predictions)\n",
    "            print(\"  valid loss  = \"+str(loss/num_batches))\n",
    "            print(\"  valid acc   = \"+str(np.nanmean(correct)))\n",
    "            print(\"  valid AUROC = \"+str(np.nanmean(auc_roc)))\n",
    "            print(\"  valid AUPRC = \"+str(np.nanmean(auc_pr)))\n",
    "\n",
    "            # check if current validation loss is lower, if so, save parameters, if not check patience\n",
    "            if valid_loss < min_loss:\n",
    "                print(\"  Lower validation loss found. Saving parameters to: \"+params_path)\n",
    "\n",
    "                # save model parameters\n",
    "                saver = tf.train.Saver()\n",
    "                saver.save(sess, save_path=params_path)\n",
    "\n",
    "                # set minimum loss to the current validation loss\n",
    "                min_loss = valid_loss\n",
    "\n",
    "                # reset wait time\n",
    "                wait = 0\n",
    "            else:\n",
    "\n",
    "                # add to wait time\n",
    "                wait += 1\n",
    "\n",
    "                # check to see if patience has run out\n",
    "                if wait == patience:\n",
    "                    print(\"Patience ran out... early stopping!\")\n",
    "                    break\n",
    "\n",
    "        # close tensorflow session (Note, the graph is still open)\n",
    "        sess.close()\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "\n",
    "    '''LOAD PARAMETERS'''\n",
    "    save_path = os.path.join(params_results, exp)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.mkdir(save_path)\n",
    "        print(\"making directory: \" + save_path)\n",
    "    params_filename = '%s_%s_best'%(modelarch, trial)\n",
    "    params_path = os.path.join(save_path, params_filename)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # restore trained parameters\n",
    "    saver = tf.train.Saver()\n",
    "    saver.restore(sess, save_path=params_path)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    '''TEST'''\n",
    "    batch_size = 128\n",
    "    batches, sort_index = helper.bucket_generator(X_test, Y_test, batch_size, index=True)\n",
    "    num_batches = len(batches)\n",
    "\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    valid_predictions = []\n",
    "    valid_truth = []\n",
    "    start_time = time.time()\n",
    "    num_batches = len(batches)\n",
    "    bar_length = 25\n",
    "\n",
    "    for i, batch in enumerate(batches):\n",
    "\n",
    "        batch_loss, batch_predict, batch_logits = sess.run([total_loss, predictions, logits], feed_dict={X: batch[0], \n",
    "                                                                                Y: batch[1], \n",
    "                                                                                keep_prob: 1.0})            \n",
    "        loss += batch_loss\n",
    "        valid_predictions.append(batch_predict)\n",
    "        valid_truth.append(batch[1])\n",
    "        \n",
    "        remaining_time = (time.time()-start_time)*(num_batches-(i+1))/(i+1)\n",
    "        percent = float(i)/num_batches\n",
    "        progress = '='*int(round(percent*bar_length))\n",
    "        spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "        sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs -- loss=%.5f -- acc=%.5f\" \\\n",
    "        %(progress+spaces, percent*100, remaining_time, loss/(i+1), acc/(i+1)))\n",
    "\n",
    "    sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs -- loss=%.5f -- acc=%.5f\\n\" \\\n",
    "    %(progress+spaces, percent*100, time.time()-start_time, loss/(i+1), acc/(i+1)))\n",
    "    sys.stdout.write(\"\\n\")\n",
    "\n",
    "    valid_predictions = np.vstack(valid_predictions)\n",
    "    valid_truth = np.vstack(valid_truth)\n",
    "\n",
    "    correct = np.mean(np.equal(valid_truth, np.round(valid_predictions)))\n",
    "    auc_roc, roc_curves = helper.roc(valid_truth, valid_predictions)\n",
    "    auc_pr, pr_curves = helper.pr(valid_truth, valid_predictions)\n",
    "    mean_vals = [np.nanmean(correct), np.nanmean(auc_roc), np.nanmean(auc_pr)]\n",
    "    std = [np.nanstd(correct), np.nanstd(auc_roc), np.nanstd(auc_pr)]\n",
    "\n",
    "    print(\"  test loss  = \"+str(loss/num_batches))\n",
    "    print(\"  test acc   = \"+str(np.nanmean(correct)))\n",
    "    print(\"  test AUROC = \"+str(np.nanmean(auc_roc)))\n",
    "    print(\"  test AUPRC = \"+str(np.nanmean(auc_pr)))\n",
    "    \n",
    "    if WRITE:\n",
    "        metricsline = '%s,%s,%s,%s,%s,%s,%s'%(exp, modelarch, trial, loss, mean_vals[0], mean_vals[1], mean_vals[2])\n",
    "        fd = open('test_metrics.csv', 'a')\n",
    "        fd.write(metricsline+'\\n')\n",
    "        fd.close()\n",
    "\n",
    "\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    '''SORT ACTIVATIONS'''\n",
    "    WT_predictions = valid_predictions[np.argsort(sort_index)]\n",
    "    plot_index = np.argsort(WT_predictions[:,0])[::-1]\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    '''SECOND ORDER MUTAGENESIS'''\n",
    "\n",
    "    '''Som Calc'''\n",
    "    if SOMCALC:\n",
    "        def second_order_mutagenesis(sess, predictions, X_val, ugidx):\n",
    "            seqlen, dims = X_val.shape\n",
    "            idxlen = len(ugidx)\n",
    "\n",
    "            # get wild-type score\n",
    "            wt_score = sess.run(predictions, feed_dict={X: np.expand_dims(X_val, axis=0), keep_prob: 1.0})[0]     \n",
    "\n",
    "            # generate mutagenesis sequences\n",
    "            num_mut = idxlen**2*dims**2\n",
    "            X_mut = np.einsum('nl,lka->nka', np.ones((num_mut, 1)), np.expand_dims(X_val, axis=0))\n",
    "\n",
    "            k=0\n",
    "            for position1 in ugidx:\n",
    "                for position2 in ugidx:\n",
    "                    for nuc1 in range(dims):\n",
    "                        for nuc2 in range(dims):\n",
    "                            X_mut[k, position1, :] = 0\n",
    "                            X_mut[k, position1, nuc1] = 1        \n",
    "                            X_mut[k, position2, :] = 0\n",
    "                            X_mut[k, position2, nuc2] = 1        \n",
    "                            k += 1\n",
    "\n",
    "            # get second order mutagenesis score\n",
    "            X_mut = [x for x in X_mut]\n",
    "            mut_scores = []\n",
    "            batches = helper.batch_generator(X_mut, batch_size=512, MAX=None, shuffle_data=False)\n",
    "            for i, batch in enumerate(batches):\n",
    "                batch_predict = sess.run(predictions, feed_dict={X: batch, keep_prob: 1.0})            \n",
    "                mut_scores.append(batch_predict)\n",
    "            mut_scores = np.vstack(mut_scores)\n",
    "            \n",
    "            # calculate log-odds score\n",
    "            log_odds = np.log(mut_scores + 1e-7) - np.log(wt_score + 1e-7)\n",
    "\n",
    "\n",
    "            # reshape second order scores\n",
    "            second_mutagenesis_logodds = np.zeros((idxlen, idxlen, dims, dims))\n",
    "            k = 0\n",
    "            for i in range(idxlen):\n",
    "                for j in range(idxlen):\n",
    "                    for m in range(dims):\n",
    "                        for n in range(dims):\n",
    "                            second_mutagenesis_logodds[i,j,m,n] = log_odds[k,0]\n",
    "                            k += 1\n",
    "            return second_mutagenesis_logodds\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "        np.random.seed(274)\n",
    "\n",
    "        bar_length = 50\n",
    "        N = 2000\n",
    "        mutagenesis_logodds = []\n",
    "        for i, index in enumerate(plot_index[:N]):\n",
    "            logresult = second_order_mutagenesis(sess, logits, X_test[index], range(seqlen))\n",
    "            mutagenesis_logodds.append(logresult)\n",
    "\n",
    "\n",
    "            remaining_time = (time.time()-start_time)*(N-(i+1))/(i+1)\n",
    "            percent = float(i)/N\n",
    "            progress = '='*int(round(percent*bar_length))\n",
    "            spaces = ' '*int(bar_length-round(percent*bar_length))\n",
    "            sys.stdout.write(\"\\r[%s] %.1f%% -- remaining time=%.2fs\" \\\n",
    "            %(progress+spaces, percent*100, remaining_time))\n",
    "\n",
    "        sys.stdout.write(\"\\r[%s] %.1f%% -- elapsed time=%.2fs\" \\\n",
    "        %(progress+spaces, percent*100, time.time()-start_time))\n",
    "        sys.stdout.write(\"\\n\")\n",
    "\n",
    "        mean_mut2 = np.nanmean(mutagenesis_logodds, axis=0)\n",
    "        idx = np.where(np.isnan(mean_mut2))\n",
    "        mean_mut2[idx] = np.min(mean_mut2)\n",
    "\n",
    "        arrayspath = 'Arrays/%s_%s%s_so.npy'%(exp, modelarch, trial)\n",
    "        np.save(arrayspath, mean_mut2)\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------\n",
    "    '''Som Vis'''\n",
    "\n",
    "    if SOMVIS:\n",
    "        arrayspath = 'Arrays/%s_%s%s_so.npy'%(exp, modelarch, trial)\n",
    "\n",
    "        mean_mut2 = np.load(arrayspath)\n",
    "\n",
    "        norm_mean_mut2 = helper.normalize_mut_hol(mean_mut2, normfactor=1)\n",
    "\n",
    "        #Let's try something weird\n",
    "        bpfilter = np.ones((4,4))*-1\n",
    "        for i,j in zip(range(4), range(4)):\n",
    "            bpfilter[i, -(j+1)] = 1.\n",
    "\n",
    "        C = np.sum((norm_mean_mut2*bpfilter).reshape(seqlen,seqlen,dims*dims), axis=2)\n",
    "        #C = C - np.mean(C)\n",
    "        #C = C/np.max(C)\n",
    "\n",
    "        color = 'Reds'\n",
    "\n",
    "        plt.figure(figsize=(8,6))\n",
    "        sb.heatmap(C, vmin=None, cmap=color , linewidth=0.00)\n",
    "        plt.title('Base Pair scores: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "        som_file = modelsavename + 'SoM_bpfilter' + '.png'\n",
    "        som_file = os.path.join(img_folder, som_file)\n",
    "        plt.savefig(som_file)\n",
    "        plt.close()\n",
    "\n",
    "        blocklen = np.sqrt(np.product(mean_mut2.shape)).astype(int)\n",
    "        S = np.zeros((blocklen, blocklen))\n",
    "        i,j,k,l = mean_mut2.shape\n",
    "\n",
    "        for ii in range(i):\n",
    "            for jj in range(j):\n",
    "                for kk in range(k):\n",
    "                    for ll in range(l):\n",
    "                        S[(4*ii)+kk, (4*jj)+ll] = mean_mut2[ii,jj,kk,ll]\n",
    "\n",
    "        plt.figure(figsize=(15,15))\n",
    "        plt.imshow(S,  cmap='RdPu')\n",
    "        plt.colorbar()\n",
    "        plt.title('Blockvis of all mutations: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "        som_file =modelsavename+ 'SoM_blockvis' + '.png'\n",
    "        som_file = os.path.join(img_folder, som_file)\n",
    "        plt.savefig(som_file)\n",
    "        plt.close()\n",
    "    #---------------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
