{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#double mutate function used in som_average_ug_split for large alignments\n",
    "def double_mutate_ungapped_split(X, ug_r, ug_c):\n",
    "\n",
    "    num_summary, seqlen, _, dims = X.shape\n",
    "    idxlen_r = len(ug_r)\n",
    "    idxlen_c = len(ug_c)\n",
    "\n",
    "    mutations_matrix = np.zeros((idxlen_r,idxlen_c, dims*dims, seqlen,1,dims))\n",
    "\n",
    "    for i1,position1 in enumerate(ug_r):\n",
    "\n",
    "        for i2,position2 in enumerate(ug_c):\n",
    "\n",
    "            for nuc1 in range(dims):\n",
    "\n",
    "                for nuc2 in range(dims):\n",
    "\n",
    "                    mut_seq = np.copy(X)\n",
    "                    mut_seq[0, position1, 0, :] = np.zeros(dims)\n",
    "                    mut_seq[0, position1, 0, nuc1] = 1.0\n",
    "                    mut_seq[0, position2, 0, :] = np.zeros(dims)\n",
    "                    mut_seq[0, position2, 0, nuc2] = 1.0\n",
    "\n",
    "                    mutations_matrix[i1, i2, (nuc1*dims)+nuc2, :] = mut_seq\n",
    "\n",
    "    return mutations_matrix\n",
    "\n",
    "#passing in the whole list and return a list of the list split up\n",
    "def split_list(L, numsplit=4):\n",
    "    splitidx = [(len(L)//numsplit)*i for i in range(numsplit)]\n",
    "    splitidx.append(len(L))\n",
    "    splitup = [L[splitidx[f]:splitidx[f+1]] for f in range(numsplit)]\n",
    "    return (splitup, splitup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mofidied Generic that allows a single seq SoM to be broken up\n",
    "def som_average_ungapped_split(Xdict, ungapped_index, savepath, nntrainer, sess, split=4, progress='on', save=True, layer='output',\n",
    "                         normalize=False):\n",
    "\n",
    "    num_summary, seqlen, _, dims = Xdict.shape\n",
    "\n",
    "    starttime = time.time()\n",
    "\n",
    "    idxlen = len(ungapped_index)\n",
    "\n",
    "    #initialize the full array to hold all the scores before averaging\n",
    "    sum_mut2_scores = np.zeros((num_summary, idxlen, idxlen, dims, dims))\n",
    "\n",
    "    for ii in range(num_summary):\n",
    "        if progress == 'on':\n",
    "            print (ii)\n",
    "\n",
    "        epoch_starttime = time.time()\n",
    "\n",
    "        #extract sequence\n",
    "        X = np.expand_dims(Xdict[ii], axis=0)\n",
    "        #Get WT score\n",
    "        WT = {'inputs': X, 'targets': np.ones((X.shape[0], 1))}\n",
    "        WT_score = nntrainer.get_activations(sess, WT, layer=layer)[0]\n",
    "\n",
    "        #initialize temp array per sequence\n",
    "        temp_scores = np.zeros((idxlen, idxlen, dims, dims))\n",
    "        #split up the idxlen into managebale bits\n",
    "        idxs_r, idxs_c = split_list(ungapped_index, numsplit=split)\n",
    "\n",
    "        #initialize the start index of the temp array\n",
    "        start_r =0\n",
    "        for pi,r in enumerate(idxs_r):\n",
    "            start_c = 0\n",
    "            for pj,c in enumerate(idxs_c):\n",
    "                X_mutsecorder = double_mutate_ungapped_split(X, r, c)\n",
    "                #reshape the 6D tensor into a 4D tensor that the model can test\n",
    "                X_mutsecorder_reshape = np.reshape(X_mutsecorder, (len(r)*len(c)*dims*dims, seqlen, 1, dims))\n",
    "                mutations = {'inputs': X_mutsecorder_reshape, 'targets': np.ones((X_mutsecorder_reshape.shape[0], 1))}\n",
    "                #Get output activations for the mutations\n",
    "                mut2_scores= nntrainer.get_activations(sess, mutations, layer=layer)\n",
    "\n",
    "                #logodds regime:\n",
    "                if normalize == 'logodds':\n",
    "                    minscore = np.min(mut2_scores)\n",
    "                    #mut2_scores = np.log(np.clip(mut2_scores, a_min=0., a_max=1e7) + 1e-7) - np.log(WT_score+1e-7)\n",
    "                    mut2_scores = np.log(mut2_scores - minscore + 1e-7) - np.log(WT_score-minscore+1e-7)\n",
    "                #Reshape and add back to temp array\n",
    "                \n",
    "                temp_scores[start_r:start_r+len(r), start_c:start_c+len(c), :, :] = mut2_scores.reshape(len(r),len(c),dims,dims)\n",
    "                #update the temp array start index\n",
    "                start_c = start_c+len(c)\n",
    "            start_r = start_r+len(r)\n",
    "                \n",
    "        #Add the temp to the full scores\n",
    "        sum_mut2_scores[ii] = temp_scores\n",
    "\n",
    "        epoch_endtime = time.time()\n",
    "\n",
    "        if progress == 'on':\n",
    "\n",
    "            print ('Epoch duration =' + mf.sectotime(epoch_endtime -epoch_starttime))\n",
    "            print ('Cumulative duration =' + mf.sectotime(epoch_endtime - starttime))\n",
    "            print ()\n",
    "\n",
    "        if progress == 'short':\n",
    "            if ii%100 == 0:\n",
    "                print (ii)\n",
    "                print ('Epoch duration =' + mf.sectotime((epoch_endtime -epoch_starttime)*100))\n",
    "                print ('Cumulative duration =' + mf.sectotime(epoch_endtime - starttime))\n",
    "                print ()\n",
    "\n",
    "    print ('----------------Summing complete----------------')\n",
    "\n",
    "    mean_mut2_scores = np.nanmean(sum_mut2_scores, axis=0)\n",
    "\n",
    "    # Save the summed array for future use\n",
    "    if save == True:\n",
    "        np.save(savepath, mean_mut2_scores)\n",
    "        print ('Saving scores to ' + savepath)\n",
    "\n",
    "\n",
    "    return (mean_mut2_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and dict construction completed in: 0.05s\n",
      "loading model from:  ../../results/trnasec/mlp_trnasec_full_best.ckpt\n",
      "INFO:tensorflow:Restoring parameters from ../../results/trnasec/mlp_trnasec_full_best.ckpt\n",
      "2\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 25 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 25 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 25 4 4\n",
      "25 22 4 4\n",
      "25 22 4 4\n",
      "25 22 4 4\n",
      "25 25 4 4\n",
      "0\n",
      "Epoch duration =1min 38.269999999999996s\n",
      "Cumulative duration =0.98s\n",
      "\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 25 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 25 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 22 4 4\n",
      "22 25 4 4\n",
      "25 22 4 4\n",
      "25 22 4 4\n",
      "25 22 4 4\n",
      "25 25 4 4\n",
      "----------------Summing complete----------------\n",
      "Saving scores to Arrays/trnasec_mlptrnasec_full_so0k.npy\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "import helper\n",
    "from deepomics import neuralnetwork as nn\n",
    "from deepomics import utils, fit, visualize, saliency\n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE ACTIONS'''\n",
    "TRAIN = False\n",
    "TEST = False\n",
    "WRITE = False\n",
    "FOM = False\n",
    "SOMCALC = True\n",
    "SOMVIS = False\n",
    "\n",
    "if '--train' in sys.argv:\n",
    "  TRAIN = True\n",
    "if '--test' in sys.argv:\n",
    "  TEST = True\n",
    "if '--write' in sys.argv:\n",
    "  WRITE = True\n",
    "if '--fom' in sys.argv:\n",
    "  FOM = True\n",
    "if '--somcalc' in sys.argv:\n",
    "  SOMCALC = True\n",
    "if '--somvis' in sys.argv:\n",
    "  SOMVIS = True\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE LOOP'''\n",
    "\n",
    "exp = 'trnasec'  #for the params folder\n",
    "\n",
    "\n",
    "img_folder = 'Images_mlp'\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''OPEN DATA'''\n",
    "\n",
    "starttime = time.time()\n",
    "\n",
    "#Open data from h5py\n",
    "filename = 'trnasec_full.hdf5'\n",
    "with h5py.File(filename, 'r') as dataset:\n",
    "    X_data = np.array(dataset['X_data'])\n",
    "    Y_data = np.array(dataset['Y_data'])\n",
    "\n",
    "numdata, seqlen, _, dims = X_data.shape\n",
    "dims = dims-1\n",
    "\n",
    "#remove gaps from sequences\n",
    "ungapped = True\n",
    "if ungapped:\n",
    "    X_data = X_data[:, :, :, :dims]\n",
    "\n",
    "# get validation and test set from training set\n",
    "train_frac = 0.8\n",
    "valid_frac = 0.1\n",
    "test_frac = 1-0.8-valid_frac\n",
    "N = numdata\n",
    "posidx = np.random.permutation(np.arange(N//2))\n",
    "negidx = np.random.permutation(np.arange(N//2, N))\n",
    "split_1 = int((N//2)*(1-valid_frac-test_frac))\n",
    "split_2 = int((N//2)*(1-test_frac))\n",
    "#shuffle = np.random.permutation(N)\n",
    "\n",
    "trainidx = np.random.permutation(np.concatenate([posidx[:split_1], negidx[:split_1]]))\n",
    "valididx = np.random.permutation(np.concatenate([posidx[split_1:split_2], negidx[split_1:split_2]]))\n",
    "testidx = np.random.permutation(np.concatenate([posidx[split_2:], negidx[split_2:]]))\n",
    "\n",
    "#set up dictionaries\n",
    "train = {'inputs': X_data[trainidx],\n",
    "         'targets': Y_data[trainidx]}\n",
    "valid = {'inputs': X_data[valididx],\n",
    "         'targets': Y_data[valididx]}\n",
    "test = {'inputs': X_data[testidx],\n",
    "         'targets': Y_data[testidx]}\n",
    "\n",
    "print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))\n",
    "\n",
    "simalign_file = 'trnasec_full.sto'\n",
    "#Get the full secondary structure and sequence consensus from the emission\n",
    "SS = mf.getSSconsensus(simalign_file)\n",
    "SQ = mf.getSQconsensus(simalign_file)\n",
    "\n",
    "#Get the ungapped sequence and the indices of ungapped nucleotides\n",
    "_, ugSS, ugidx = mf.rm_consensus_gaps(X_data, SS)\n",
    "_, ugSQ, _ = mf.rm_consensus_gaps(X_data, SQ)\n",
    "\n",
    "\n",
    "#Get the sequence and indices of the conserved base pairs\n",
    "bpchars = ['(',')','<','>','{','}']\n",
    "sig_bpchars = ['<','>']\n",
    "bpidx, bpSS, nonbpidx = mf.sigbasepair(SS, bpchars)\n",
    "numbp = len(bpidx)\n",
    "numug = len(ugidx)\n",
    "\n",
    "#Get the bpug information\n",
    "bpugSQ, bpugidx = mf.bpug(ugidx, bpidx, SQ)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "'''SAVE PATHS AND PARAMETERS'''\n",
    "params_results = '../../results'\n",
    "\n",
    "modelarch = 'mlp'\n",
    "trial = 'trnasec_full'\n",
    "modelsavename = '%s_%s'%(modelarch, trial)\n",
    "\n",
    "\n",
    "\n",
    "'''BUILD NEURAL NETWORK'''\n",
    "\n",
    "def cnn_model(input_shape, output_shape):\n",
    "\n",
    "\n",
    "  # create model\n",
    "  layer1 = {'layer': 'input', #41\n",
    "          'input_shape': input_shape\n",
    "          }\n",
    "\n",
    "  layer2 = {'layer': 'dense',        # input, conv1d, dense, conv1d_residual, dense_residual, conv1d_transpose,\n",
    "                                      # concat, embedding, variational_normal, variational_softmax, + more\n",
    "            'num_units': 44,\n",
    "            'norm': 'batch',          # if removed, automatically adds bias instead\n",
    "            'activation': 'relu',     # or leaky_relu, prelu, sigmoid, tanh, etc\n",
    "            'dropout': 0.5,           # if removed, default is no dropout\n",
    "           }\n",
    "\n",
    "  layer3 = {'layer': 'dense',\n",
    "          'num_units': output_shape[1],\n",
    "          'activation': 'sigmoid'\n",
    "          }\n",
    "\n",
    "  model_layers = [layer1, layer2, layer3]\n",
    "\n",
    "  # optimization parameters\n",
    "  optimization = {\"objective\": \"binary\",\n",
    "                \"optimizer\": \"adam\",\n",
    "                \"learning_rate\": 0.0003,\n",
    "                \"l2\": 1e-5,\n",
    "                #\"label_smoothing\": 0.05,\n",
    "                #\"l1\": 1e-6,\n",
    "                }\n",
    "  return model_layers, optimization\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# get shapes of inputs and targets\n",
    "input_shape = list(train['inputs'].shape)\n",
    "input_shape[0] = None\n",
    "output_shape = train['targets'].shape\n",
    "\n",
    "# load model parameters\n",
    "model_layers, optimization = cnn_model(input_shape, output_shape)\n",
    "\n",
    "# build neural network class\n",
    "nnmodel = nn.NeuralNet(seed=247)\n",
    "nnmodel.build_layers(model_layers, optimization)\n",
    "\n",
    "# compile neural trainer\n",
    "save_path = os.path.join(params_results, exp)\n",
    "param_path = os.path.join(save_path, modelsavename)\n",
    "nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=param_path)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "'''TRAIN '''\n",
    "if TRAIN:\n",
    "  # initialize session\n",
    "  sess = utils.initialize_session()\n",
    "\n",
    "  #Train the model\n",
    "\n",
    "  data = {'train': train, 'valid': valid}\n",
    "  fit.train_minibatch(sess, nntrainer, data,\n",
    "                    batch_size=100,\n",
    "                    num_epochs=100,\n",
    "                    patience=100,\n",
    "                    verbose=2,\n",
    "                    shuffle=True,\n",
    "                    save_all=False)\n",
    "\n",
    "\n",
    "  sess.close()\n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''TEST'''\n",
    "sess = utils.initialize_session()\n",
    "if TEST:\n",
    "\n",
    "  # set best parameters\n",
    "  nntrainer.set_best_parameters(sess)\n",
    "\n",
    "  # test model\n",
    "  loss, mean_vals, std_vals = nntrainer.test_model(sess, test, name='test')\n",
    "  if WRITE:\n",
    "    metricsline = '%s,%s,%s,%s,%s,%s,%s'%(exp, modelarch, trial, loss, mean_vals[0], mean_vals[1], mean_vals[2])\n",
    "    fd = open('test_metrics.csv', 'a')\n",
    "    fd.write(metricsline+'\\n')\n",
    "    fd.close()\n",
    "'''SORT ACTIVATIONS'''\n",
    "nntrainer.set_best_parameters(sess)\n",
    "predictionsoutput = nntrainer.get_activations(sess, test, layer='output')\n",
    "plot_index = np.argsort(predictionsoutput[:,0])[::-1]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''FIRST ORDER MUTAGENESIS'''\n",
    "if FOM:\n",
    "  plots = 3\n",
    "  num_plots = range(plots)\n",
    "  fig = plt.figure(figsize=(15,plots*2+1))\n",
    "  for ii in num_plots:\n",
    "\n",
    "      X = np.expand_dims(test['inputs'][plot_index[10000+ii]], axis=0)\n",
    "\n",
    "      ax = fig.add_subplot(plots, 1, ii+1)\n",
    "      mf.fom_saliency_mul(X, layer='dense_1_bias', alphabet='rna', nntrainer=nntrainer, sess=sess, ax =ax)\n",
    "      fom_file = modelsavename + 'FoM' + '.png'\n",
    "  fom_file = os.path.join(img_folder, fom_file)\n",
    "  plt.savefig(fom_file)\n",
    "\n",
    "  plt.close()\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''SECOND ORDER MUTAGENESIS'''\n",
    "\n",
    "'''Som calc'''\n",
    "if SOMCALC:\n",
    "  num_summary = 2 #np.min([500,len(test['inputs'])//2])\n",
    "  print (num_summary)\n",
    "\n",
    "  arrayspath = 'Arrays/%s_%s%s_so%.0fk.npy'%(exp, modelarch, trial, num_summary/1000)\n",
    "  Xdict = test['inputs'][plot_index[:num_summary]]\n",
    "\n",
    "  mean_mut2 = som_average_ungapped_split(Xdict, ugidx, arrayspath, nntrainer, sess, split=4, progress='short',\n",
    "                                             save=True, layer='dense_1_bias')\n",
    "\n",
    "if SOMVIS:\n",
    "  #Load the saved data\n",
    "  num_summary = 2 #np.min([500,len(test['inputs'])//2])\n",
    "  arrayspath = 'Arrays/%s_%s%s_so%.0fk.npy'%(exp, modelarch, trial, num_summary/1000)\n",
    "  mean_mut2 = np.load(arrayspath)\n",
    "\n",
    "  #Reshape into a holistic tensor organizing the mutations into 4*4\n",
    "  meanhol_mut2 = mean_mut2.reshape(numug,numug,4,4)\n",
    "\n",
    "  #Normalize\n",
    "  normalize = True\n",
    "  if normalize:\n",
    "      norm_meanhol_mut2 = mf.normalize_mut_hol(meanhol_mut2, nntrainer, sess, normfactor=1)\n",
    "\n",
    "  #Let's try something weird\n",
    "  bpfilter = np.ones((4,4))*0.\n",
    "  for i,j in zip(range(4), range(4)):\n",
    "      bpfilter[i, -(j+1)] = +1.\n",
    "\n",
    "  nofilter = np.ones((4,4))\n",
    "\n",
    "  C = (norm_meanhol_mut2*bpfilter)\n",
    "  C = np.sum((C).reshape(numug,numug,dims*dims), axis=2)\n",
    "  C = C - np.mean(C)\n",
    "  C = C/np.max(C)\n",
    "\n",
    "  plt.figure(figsize=(8,6))\n",
    "  sb.heatmap(C,vmin=None, cmap='Blues', linewidth=0.0)\n",
    "  plt.title('Base Pair scores: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "  som_file = modelsavename + 'SoM_bpfilter_train' + '.png'\n",
    "  som_file = os.path.join(img_folder, som_file)\n",
    "  plt.savefig(som_file)\n",
    "  plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[15.50099802, 15.68667603, 15.21925163, 16.57904863],\n",
       "         [15.50099802, 15.68667603, 15.21925163, 16.57904863],\n",
       "         [15.50099802, 15.68667603, 15.21925163, 16.57904863],\n",
       "         [15.50099802, 15.68667603, 15.21925163, 16.57904863]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       ...,\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]],\n",
       "\n",
       "\n",
       "       [[[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]],\n",
       "\n",
       "        [[ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
       "         [ 0.        ,  0.        ,  0.        ,  0.        ]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_mut2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ugidx)//4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
