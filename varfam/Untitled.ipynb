{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data extraction and dict construction completed in: 18.5s\n",
      "Epoch 1 out of 2 \n",
      "[=======                       ] 22.5% -- remaining time=9s -- loss=0.39071 -- accuracy=83.87%   "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6f3cbf65180a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    221\u001b[0m                       \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                       \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 223\u001b[0;31m                       save_all=False)\n\u001b[0m\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/steffan/deepomics/fit.pyc\u001b[0m in \u001b[0;36mtrain_minibatch\u001b[0;34m(sess, nntrainer, data, batch_size, num_epochs, patience, verbose, shuffle, save_all, save_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                                                                                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \t\t\t\t\t\t\t\t\t\t\tshuffle=shuffle)\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0;31m# test current model with cross-validation data and store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/steffan/deepomics/neuralnetwork.pyc\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(self, sess, data, batch_size, verbose, shuffle)\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_batches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_feed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_minibatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplaceholders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m                         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_calc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m                         \u001b[0mmetric\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                         \u001b[0mperformance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/peter/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/peter/anaconda2/envs/tensorflow/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os, sys, h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "import tensorflow as tf\n",
    "import scipy\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../..')\n",
    "import mutagenesisfunctions as mf\n",
    "import helper \n",
    "from deepomics import neuralnetwork as nn\n",
    "from deepomics import utils, fit, visualize, saliency\n",
    "\n",
    "from Bio import AlignIO\n",
    "import time as time\n",
    "import pandas as pd\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE ACTIONS'''\n",
    "TRAIN = True\n",
    "TEST = False\n",
    "WRITE = False\n",
    "FOM = False\n",
    "SOMCALC = False\n",
    "SOMVIS = False\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------------------------\n",
    "'''DEFINE LOOP'''\n",
    "trials = ['glna', 'trna', 'riboswitch']\n",
    "\n",
    "datafiles = {'glna': ['glna_100k_d8.hdf5', '../../data_RFAM/glnAsim_100k.sto'], \n",
    "              'trna': ['trna_100k_d4.hdf5', '../../../data_RFAM/trnasim_100k.sto'],\n",
    "              'riboswitch': ['riboswitch_100k_d4.hdf5', '../../../data_RFAM/riboswitch_100k.sto'],}\n",
    "\n",
    "exp = 'varfam'  #for both the data folder and the params folder\n",
    "exp_data = 'data_RFAM'\n",
    "\n",
    "img_folder = 'Images'\n",
    "\n",
    "for t in trials:\n",
    "\n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  '''OPEN DATA'''\n",
    "\n",
    "  starttime = time.time()\n",
    "\n",
    "  #Open data from h5py\n",
    "  filename = datafiles[t][0]\n",
    "  data_path = os.path.join('../..', exp_data, filename)\n",
    "  with h5py.File(data_path, 'r') as dataset:\n",
    "      X_data = np.array(dataset['X_data'])\n",
    "      Y_data = np.array(dataset['Y_data'])\n",
    "      \n",
    "  numdata, seqlen, _, dims = X_data.shape\n",
    "  dims = dims-1\n",
    "\n",
    "  #remove gaps from sequences\n",
    "  ungapped = True\n",
    "  if ungapped:\n",
    "      X_data = X_data[:, :, :, :dims]\n",
    "      \n",
    "  # get validation and test set from training set\n",
    "  test_frac = 0.3\n",
    "  valid_frac = 0.1\n",
    "  N = numdata\n",
    "  split_1 = int(N*(1-valid_frac-test_frac))\n",
    "  split_2 = int(N*(1-test_frac))\n",
    "  shuffle = np.random.permutation(N)\n",
    "\n",
    "  def unalign(X):\n",
    "    nuc_index = np.where(np.sum(X, axis=2)!=0)\n",
    "    return (X[nuc_index])\n",
    "\n",
    "  X_data_unalign = [unalign(X) for X in X_data]\n",
    "  maxlength = helper.get_maxlength(X_data_unalign)\n",
    "  X_data_unalign, _ = helper.pad_inputs(X_data_unalign, MAX=maxlength)\n",
    "  X_data_unalign = np.expand_dims(X_data_unalign, axis=2)\n",
    "\n",
    "  #set up dictionaries\n",
    "  train = {'inputs': X_data_unalign[shuffle[:split_1]], \n",
    "           'targets': Y_data[shuffle[:split_1]]}\n",
    "  valid = {'inputs': X_data_unalign[shuffle[split_1:split_2]], \n",
    "           'targets': Y_data[shuffle[split_1:split_2]]}\n",
    "  test = {'inputs': X_data_unalign[shuffle[split_2:]], \n",
    "           'targets': Y_data[shuffle[split_2:]]}\n",
    "\n",
    "  #set up dictionaries\n",
    "  train_align = {'inputs': X_data[shuffle[:split_1]], \n",
    "           'targets': Y_data[shuffle[:split_1]]}\n",
    "  valid_align = {'inputs': X_data[shuffle[split_1:split_2]], \n",
    "           'targets': Y_data[shuffle[split_1:split_2]]}\n",
    "  test_align = {'inputs': X_data[shuffle[split_2:]], \n",
    "           'targets': Y_data[shuffle[split_2:]]}\n",
    "      \n",
    "  print ('Data extraction and dict construction completed in: ' + mf.sectotime(time.time() - starttime))\n",
    "\n",
    "  simalign_file = datafiles[t][1]\n",
    "  #Get the full secondary structure and sequence consensus from the emission\n",
    "  SS = mf.getSSconsensus(simalign_file)\n",
    "  SQ = mf.getSQconsensus(simalign_file)\n",
    "\n",
    "  #Get the ungapped sequence and the indices of ungapped nucleotides\n",
    "  _, ugSS, ugidx = mf.rm_consensus_gaps(X_data, SS)\n",
    "  _, ugSQ, _ = mf.rm_consensus_gaps(X_data, SQ)\n",
    "\n",
    "\n",
    "  #Get the sequence and indices of the conserved base pairs\n",
    "  bpchars = ['(',')','<','>','{','}']\n",
    "  sig_bpchars = ['<','>']\n",
    "  bpidx, bpSS, nonbpidx = mf.sigbasepair(SS, bpchars)\n",
    "  numbp = len(bpidx)\n",
    "  numug = len(ugidx)\n",
    "\n",
    "  #Get the bpug information\n",
    "  bpugSQ, bpugidx = mf.bpug(ugidx, bpidx, SQ)\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "  '''SAVE PATHS AND PARAMETERS'''\n",
    "  params_results = '../../results'\n",
    "\n",
    "  modelarch = 'resbind'\n",
    "  trial = t\n",
    "  modelsavename = '%s_%s'%(modelarch, trial)\n",
    "\n",
    "\n",
    "\n",
    "  '''BUILD NEURAL NETWORK'''\n",
    "\n",
    "  def cnn_model(input_shape, output_shape):\n",
    "\n",
    "      # create model\n",
    "      layer1 = {'layer': 'input', #41\n",
    "              'input_shape': input_shape\n",
    "              }\n",
    "      layer2 = {'layer': 'conv1d',\n",
    "              'num_filters': 96,\n",
    "              'filter_size': input_shape[1]-29,\n",
    "              'norm': 'batch',\n",
    "              'activation': 'relu',\n",
    "              'dropout': 0.3,\n",
    "              'padding': 'VALID',\n",
    "              }\n",
    "      layer3 = {'layer': 'conv1d_residual',\n",
    "              'filter_size': 5,\n",
    "              'function': 'relu',\n",
    "              'dropout_block': 0.1,\n",
    "              'dropout': 0.3,\n",
    "              'mean_pool': 10,\n",
    "              }\n",
    "      \n",
    "      layer4 = {'layer': 'dense',        # input, conv1d, dense, conv1d_residual, dense_residual, conv1d_transpose,\n",
    "                                      # concat, embedding, variational_normal, variational_softmax, + more\n",
    "            'num_units': 196,\n",
    "            'norm': 'batch',          # if removed, automatically adds bias instead\n",
    "            'activation': 'relu',     # or leaky_relu, prelu, sigmoid, tanh, etc\n",
    "            'dropout': 0.5,           # if removed, default is no dropout\n",
    "               }\n",
    "\n",
    "      \n",
    "      layer5 = {'layer': 'dense',\n",
    "              'num_units': output_shape[1],\n",
    "              'activation': 'sigmoid'\n",
    "              }\n",
    "\n",
    "      model_layers = [layer1, layer2, layer3, layer4, layer5]\n",
    "\n",
    "      # optimization parameters\n",
    "      optimization = {\"objective\": \"binary\",\n",
    "                    \"optimizer\": \"adam\",\n",
    "                    \"learning_rate\": 0.0003,\n",
    "                    \"l2\": 1e-5,\n",
    "                    #\"label_smoothing\": 0.05,\n",
    "                    #\"l1\": 1e-6,\n",
    "                    }\n",
    "      return model_layers, optimization\n",
    "\n",
    "  tf.reset_default_graph()\n",
    "\n",
    "  # get shapes of inputs and targets\n",
    "  input_shape = list(train['inputs'].shape)\n",
    "  input_shape[0] = None\n",
    "  output_shape = train['targets'].shape\n",
    "\n",
    "  # load model parameters\n",
    "  model_layers, optimization = cnn_model(input_shape, output_shape)\n",
    "\n",
    "  # build neural network class\n",
    "  nnmodel = nn.NeuralNet(seed=247)\n",
    "  nnmodel.build_layers(model_layers, optimization)\n",
    "\n",
    "  # compile neural trainer\n",
    "  save_path = os.path.join(params_results, exp)\n",
    "  param_path = os.path.join(save_path, modelsavename)\n",
    "  nntrainer = nn.NeuralTrainer(nnmodel, save='best', file_path=param_path)\n",
    "\n",
    "  sess = utils.initialize_session()\n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "  '''TRAIN '''\n",
    "  if TRAIN:\n",
    "    # initialize session\n",
    "    sess = utils.initialize_session()\n",
    "\n",
    "    #Train the model\n",
    "\n",
    "    data = {'train': train, 'valid': valid}\n",
    "    fit.train_minibatch(sess, nntrainer, data, \n",
    "                      batch_size=100, \n",
    "                      num_epochs=2,\n",
    "                      patience=40, \n",
    "                      verbose=2, \n",
    "                      shuffle=True, \n",
    "                      save_all=False)\n",
    "\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "    #---------------------------------------------------------------------------------------------------------------------------------      \n",
    "  '''TEST'''\n",
    "  if TEST:\n",
    "    \n",
    "    # set best parameters\n",
    "    nntrainer.set_best_parameters(sess)\n",
    "\n",
    "    # test model\n",
    "    loss, mean_vals, std_vals = nntrainer.test_model(sess, test, name='test')\n",
    "    if WRITE:\n",
    "      metricsline = '%s,%s,%s,%s,%s,%s,%s'%(exp, modelarch, trial, loss, mean_vals[0], mean_vals[1], mean_vals[2])\n",
    "      fd = open('test_metrics.csv', 'a')\n",
    "      fd.write(metricsline+'\\n')\n",
    "      fd.close()\n",
    "  '''SORT ACTIVATIONS'''\n",
    "  nntrainer.set_best_parameters(sess)\n",
    "  predictionsoutput = nntrainer.get_activations(sess, test, layer='output')\n",
    "  plot_index = np.argsort(predictionsoutput[:,0])[::-1]\n",
    "\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "  '''FIRST ORDER MUTAGENESIS'''\n",
    "  if FOM:\n",
    "    num_plots = range(1)\n",
    "    for ii in num_plots: \n",
    "\n",
    "        X = np.expand_dims(test['inputs'][plot_index[10000+ii]], axis=0)\n",
    "        \n",
    "        mf.fom_saliency(X, layer='dense_1_bias', alphabet='rna', nntrainer=nntrainer, sess=sess, figsize=(15,1.5))\n",
    "        fom_file = modelsavename + 'FoM' + '.png'\n",
    "        fom_file = os.path.join(img_folder, fom_file)\n",
    "        plt.savefig(fom_file)\n",
    "\n",
    "    plt.close()\n",
    "  #---------------------------------------------------------------------------------------------------------------------------------\n",
    "  '''SECOND ORDER MUTAGENESIS'''\n",
    "\n",
    "  '''Som calc'''\n",
    "  if SOMCALC:\n",
    "    num_summary = 2\n",
    "\n",
    "    arrayspath = 'Arrays/%s_%s%s_so%.0fk.npy'%(exp, modelarch, trial, num_summary/1000)\n",
    "    X = test_align['inputs'][plot_index[:num_summary]]\n",
    "\n",
    "    mean_mut2 = helper.som_average_ungapped_logodds_unalign(X, ugidx,maxlength, arrayspath, nntrainer, sess, progress='short', \n",
    "                                               save=True, layer='dense_1_bias')\n",
    "\n",
    "  if SOMVIS:  \n",
    "    #Load the saved data\n",
    "    num_summary = 2\n",
    "    arrayspath = 'Arrays/%s_%s%s_so%.0fk.npy'%(exp, modelarch, trial, num_summary/1000)\n",
    "    mean_mut2 = np.load(arrayspath)\n",
    "\n",
    "    #Reshape into a holistic tensor organizing the mutations into 4*4\n",
    "    meanhol_mut2 = mean_mut2.reshape(numug,numug,4,4)\n",
    "\n",
    "    #Normalize\n",
    "    normalize = True\n",
    "    if normalize:\n",
    "        norm_meanhol_mut2 = mf.normalize_mut_hol(meanhol_mut2, nntrainer, sess, normfactor=1)\n",
    "\n",
    "    #Let's try something weird\n",
    "    bpfilter = np.ones((4,4))*0.\n",
    "    for i,j in zip(range(4), range(4)):\n",
    "        bpfilter[i, -(j+1)] = +1.\n",
    "\n",
    "    nofilter = np.ones((4,4))\n",
    "\n",
    "    C = (norm_meanhol_mut2*bpfilter)\n",
    "    C = np.sum((C).reshape(numug,numug,dims*dims), axis=2)\n",
    "    #C = C - np.mean(C)\n",
    "    #C = C/np.max(C)\n",
    "\n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1,2,1)\n",
    "    sb.heatmap(C,vmin=None, cmap='Blues', linewidth=0.0)\n",
    "    plt.title('Base Pair scores: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "    som_file = modelsavename + 'SoM_bpfilter' + '.png'\n",
    "    som_file = os.path.join(img_folder, som_file)\n",
    "    plt.savefig(som_file)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    blocklen = np.sqrt(np.product(meanhol_mut2.shape)).astype(int)\n",
    "    S = np.zeros((blocklen, blocklen))\n",
    "    i,j,k,l = meanhol_mut2.shape\n",
    "\n",
    "    for ii in range(i):\n",
    "        for jj in range(j):\n",
    "            for kk in range(k):\n",
    "                for ll in range(l):\n",
    "                    S[(4*ii)+kk, (4*jj)+ll] = meanhol_mut2[ii,jj,kk,ll]\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.imshow(S,  cmap='Reds', vmin=None)\n",
    "    plt.colorbar()\n",
    "    plt.title('Blockvis of all mutations: %s %s %s'%(exp, modelarch, trial))\n",
    "\n",
    "    som_file = modelsavename + 'SoM_blockvis' + '.png'\n",
    "    som_file = os.path.join(img_folder, som_file)\n",
    "    plt.savefig(som_file)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200000, 367, 1, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_data_unalign, _ = helper.pad_inputs(X_data_unalign,MAX=maxlength)\n",
    "X_data_unalign = np.expand_dims(X_data_unalign, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = [unalign(X) for X in X_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(u[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
